{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Semantics Model: Syntactic-Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/BASIC_TOPICS/DISTRIBUTIONAL_SEMANTICS/ASSIGNMENT_03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "stopwords = stopwords.words('english')\n",
    "from collections import defaultdict, Counter\n",
    "from functools import partial\n",
    "from itertools import permutations, product\n",
    "from string import punctuation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LoadWiki:\n",
    "    # load wiki dump in the following format:\n",
    "    # [ ([(rel, dependended, dependant), (..), ...], \n",
    "    #    [(tk, tk_lemma, pos), (..), ...]),\n",
    "    #   ([...], [...]), \n",
    "    #   ... ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.raw = []\n",
    "        with open('wikicorpus.txt','rb') as f:\n",
    "            bufferList = [] # temporarily store dependencies until encounter <c>.\n",
    "            for line in f.readlines():\n",
    "                if line.startswith('('):\n",
    "                    bufferList.append(self.__dependency_line_processor(line))\n",
    "                elif line.startswith('<c>'):\n",
    "                    self.raw.append((bufferList, self.__pos_line_processor(line)))\n",
    "                    bufferList = []\n",
    "                else: pass\n",
    "    \n",
    "    def __dependency_line_processor(self, line):\n",
    "        if line[-2]=='_': # get rid of parentheses in (rel,dpdd,dpdt,_) case.\n",
    "            tmp = line[1:-2].split()\n",
    "            return (tmp[0],tmp[1].split('_')[0],tmp[2].split('_')[0])\n",
    "        else: # get rid of parentheses in (rel,_,dpdd,dpdt), (rel,dpdd,dpdt), (rel,dpdd,dpdt,..), (..) case.\n",
    "            tmp = line[1:-1].split()\n",
    "            if len(tmp)<3: return\n",
    "            elif len(tmp)==3 or len(tmp)==4: return (tmp[0],tmp[1].split('_')[0],tmp[2].split('_')[0])\n",
    "            else: \n",
    "                return (tmp[0],tmp[-2].split('_')[0],tmp[-1].split('_')[0]) # format: (rel, depended, dependant)\n",
    "    \n",
    "    def __term_splitter(self, term):\n",
    "        tmp = term.split('|')\n",
    "        return (tmp[0],tmp[1],tmp[2]) # save only tk, lm, pos in a tuple.\n",
    "    \n",
    "    def __pos_line_processor(self, line):\n",
    "        tmp = line[4:].split() # get rid of initial <c>\n",
    "        return map(self.__term_splitter, tmp) \n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34 s, sys: 2.31 s, total: 36.3 s\n",
      "Wall time: 36.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = LoadWiki().get_data() # the number of sents = 397238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dependencies = [depList for depList,sentList in data]\n",
    "sents = [sentList for depList,sentList in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# GET RID OF SENTS W/ EMPTY DEPS\n",
    "emptyIdx = [i for i,item in enumerate(dependencies) if item==[]]\n",
    "sents = [sent for i,sent in enumerate(sents) if i not in emptyIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dependencies = [dep for dep in dependencies if dep!=[]]\n",
    "    # len(deps)=len(sents)=370645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DEPENDENCY TYPES\n",
    "depTypes = list({dep[0] for depList in dependencies for dep in depList if dep is not None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xcomp',\n",
       " 'xmod',\n",
       " 'ncsubj',\n",
       " 'xsubj',\n",
       " 'ncmod',\n",
       " 'cmod',\n",
       " 'det',\n",
       " 'dobj',\n",
       " 'obj2',\n",
       " 'iobj',\n",
       " 'aux',\n",
       " 'conj',\n",
       " 'ccomp']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(w2w):\n",
    "    w2w_norm = w2w / np.apply_along_axis(lambda r: np.sqrt(np.dot(r,r))\n",
    "                               , 1, w2w)[:,np.newaxis]\n",
    "    return np.dot(w2w_norm, w2w_norm.T)\n",
    "    \n",
    "def ppmi(w2w):\n",
    "    rowSums, colSums, totalSums = w2w.sum(axis=1), w2w.sum(axis=0), w2w.sum()\n",
    "    pwi, pwj, ppmiMatrix = rowSums/totalSums, colSums/totalSums, w2w/totalSums\n",
    "    ppmiMatrix /= pwi[:,np.newaxis] # * 1/pwi by row.\n",
    "    ppmiMatrix /= pwj # * 1/pwj by col.\n",
    "    ppmiMatrix = np.nan_to_num(np.log(ppmiMatrix)) # compute pmi.\n",
    "    ppmiMatrix = np.maximum(ppmiMatrix, 0) # compute ppmi.\n",
    "    return ppmiMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DepedencyDict:\n",
    "    \n",
    "    def __init__(self, dependencies, sents):\n",
    "        self.dependencies = dependencies\n",
    "        self.sents = sents\n",
    "        self.depDict = defaultdict(list)\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.__build_dependency_dict()\n",
    "        \n",
    "    def __build_dependency_dict(self):\n",
    "        print \"... extracting dependencies\"\n",
    "        for i,sent in enumerate(sents):\n",
    "            for word in sent:\n",
    "                if word[2].startswith('N'):\n",
    "                    dps = self.__extract_dependencies(i,safe(word[0]))\n",
    "                    if len(dps)!=0: self.depDict[self.stemmer.stem(safe(word[0]))].extend(dps)\n",
    "    \n",
    "    def __extract_dependencies(self, sentIndex, word):\n",
    "        dps = []\n",
    "        for dp in self.dependencies[sentIndex]:\n",
    "            if dp is not None and word in dp:\n",
    "                if word==safe(dp[1]) and dp[2] not in punctuation:\n",
    "                    dps.append(self.stemmer.stem(safe(dp[2])))\n",
    "                elif word==safe(dp[2]) and dp[1] not in punctuation:\n",
    "                    dps.append(self.stemmer.stem(safe(dp[1])))\n",
    "                else: pass\n",
    "#                 dps.append(self.stemmer.stem(safe(dp[2]))) if word==safe(dp[1]) \\\n",
    "#                     else dps.append(self.stemmer.stem(safe(dp[1])))\n",
    "            else: pass\n",
    "        return dps\n",
    "\n",
    "def safe(word):\n",
    "    return word.decode('utf-8','ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... extracting dependencies\n",
      "CPU times: user 1min 45s, sys: 171 ms, total: 1min 45s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "depDict = DepedencyDict(dependencies,sents).depDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleDistSem:\n",
    "    \n",
    "    def __init__(self, sents, depDict, kFrequent=50):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        print \"... counting word frequencies\"\n",
    "        self.freqCounts = Counter([self.stemmer.stem(safe(word[0])) for sent in sents for word in sent])\n",
    "        self.depDict = depDict\n",
    "        print \"... building vocabulary\"\n",
    "        self.vocab = [word for word in self.depDict.keys() if self.freqCounts[word]>kFrequent]\n",
    "        self.contexts = list({value for values in self.depDict.values() for value in values})\n",
    "        self.wordToIndex = {word:index for index,word in enumerate(self.vocab)}\n",
    "        self.contextToIndex = {context:index for index,context in enumerate(self.contexts)}\n",
    "    \n",
    "    def build_w2c_matrix(self):\n",
    "        \n",
    "        print \"... building full w2c matrix\"\n",
    "        self.w2c = np.zeros((len(self.vocab),len(self.contexts)))\n",
    "        counter = 0\n",
    "        for word in self.vocab:\n",
    "            counter += 1\n",
    "            for dep in self.depDict[word]:\n",
    "                self.w2c[self.wordToIndex[word]][self.contextToIndex[dep]] += 1\n",
    "            if counter % 500 == 0: print \"... processed %d words\" % counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... counting word frequencies\n",
      "... building vocabulary\n",
      "CPU times: user 1min 22s, sys: 481 ms, total: 1min 22s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = SimpleDistSem(sents,depDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building full w2c matrix\n",
      "... processed 500 words\n",
      "... processed 1000 words\n",
      "... processed 1500 words\n",
      "... processed 2000 words\n",
      "... processed 2500 words\n",
      "... processed 3000 words\n",
      "... processed 3500 words\n",
      "... processed 4000 words\n",
      "... processed 4500 words\n",
      "... processed 5000 words\n",
      "... processed 5500 words\n",
      "... processed 6000 words\n",
      "... processed 6500 words\n",
      "... processed 7000 words\n",
      "... processed 7500 words\n",
      "... processed 8000 words\n",
      "... processed 8500 words\n",
      "CPU times: user 3.52 s, sys: 381 ms, total: 3.9 s\n",
      "Wall time: 3.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_w2c_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8685, 17638)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.w2c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 33s, sys: 1.2 s, total: 1min 34s\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cosineSimilarities = cosine(ds.w2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.8 ms, sys: 67.8 ms, total: 91.6 ms\n",
      "Wall time: 89.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = ds.vocab\n",
    "wordToIndex = ds.wordToIndex\n",
    "indexToWord = {i:w for w,i in wordToIndex.iteritems()}\n",
    "words = ['car','bus','hospital','hotel','gun','bomb','horse','fox','table','bowl','guitar','piano']\n",
    "words = filter(lambda w:1 if w in vocab else 0, words)\n",
    "w2sim = {}\n",
    "for word in words:\n",
    "    simList = cosineSimilarities[wordToIndex[word]]\n",
    "    w2sim[word] = map(lambda idx:(indexToWord[idx],\n",
    "                                  cosineSimilarities[wordToIndex[word]][idx]),\n",
    "                      np.argsort(simList)[::-1][1:20+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'system', 0.97359998771879197),\n",
       " (u'structur', 0.97263485741860878),\n",
       " (u'design', 0.96832065907347264),\n",
       " (u'charact', 0.96640669846764304),\n",
       " (u'divis', 0.96638110633317631),\n",
       " (u'ship', 0.96609023407073136),\n",
       " (u'network', 0.96605111606453675),\n",
       " (u'program', 0.96516951583765997),\n",
       " (u'hous', 0.96341195016901793),\n",
       " (u'branch', 0.96229483123467285),\n",
       " (u'letter', 0.96222918429087723),\n",
       " (u'stone', 0.96079214339859165),\n",
       " (u'plot', 0.95993755015311311),\n",
       " (u'machin', 0.95992822971600389),\n",
       " (u'sound', 0.95945330361446468),\n",
       " (u'forc', 0.95932615190546255),\n",
       " (u'unit', 0.95902237068157614),\n",
       " (u'count', 0.95883709439604148),\n",
       " (u'shell', 0.9586924138689481),\n",
       " (u'class', 0.95837460062135793)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2sim['car']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def bless_evaluator(simMatrix=None, indexers=[None,None]):\n",
    "    wordToIndex, indexToWord = indexers\n",
    "    path = '/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/BASIC_TOPICS/DISTRIBUTIONAL_SEMANTICS/ASSIGNMENT_03/BLESS_part.txt'\n",
    "    with open(path,'rb') as f:\n",
    "        bless = f.readlines()\n",
    "    bless = [line.split('\\t') for line in bless] # split into (concept, _, relation, relatum).\n",
    "    \n",
    "    filter(lambda w:1 if w in vocab else 0, words)\n",
    "    \n",
    "    def filter_vocab(w):\n",
    "        if w in ds.vocab: return w # this is way too hacky, i know.\n",
    "    \n",
    "    crPairs = [(c.split('-')[0],r.split('-')[0],rel) for c,_,rel,r in bless]\n",
    "    posPairs = [(c,r) for c,r,rel in crPairs if rel=='hyper' \n",
    "                 if filter_vocab(c) is not None and filter_vocab(r) is not None]\n",
    "    negPairs = [(c,r) for c,r,rel in crPairs if rel=='mero' \n",
    "                 if filter_vocab(c) is not None and filter_vocab(r) is not None]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    return [map(lambda (c,r):(c,r,simMatrix[wordToIndex[stemmer.stem(c)]][wordToIndex[stemmer.stem(r)]]), posPairs),\n",
    "            map(lambda (c,r):(c,r,simMatrix[wordToIndex[stemmer.stem(c)]][wordToIndex[stemmer.stem(r)]]), negPairs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posEval, negEval = bless_evaluator(cosineSimilarities, indexers=[ds.wordToIndex, ds.vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Cosine): \n",
      "[('piano', 'artifact', 0.50474002855218159), ('violin', 'artifact', 0.55263562539169031), ('bowl', 'vessel', 0.85618419681912072), ('jet', 'aircraft', 0.86721147284187294), ('jet', 'craft', 0.80909679986787131)]\n",
      "Average Cosine:  0.747963592901\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(posEval, 5)\n",
    "print \"Average Cosine: \", np.mean([cosineVal for _,_,cosineVal in posEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Cosine): \n",
      "[('car', 'plate', 0.92880148980402744), ('fighter', 'top', 0.59276522238339391), ('train', 'driver', 0.82425045445738843), ('pub', 'glass', 0.76026941907066259), ('bowl', 'interior', 0.84189714921644343)]\n",
      "Average Cosine:  0.71709650429\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(negEval, 5)\n",
    "print \"Average Cosine: \", np.mean([cosineVal for _,_,cosineVal in negEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
