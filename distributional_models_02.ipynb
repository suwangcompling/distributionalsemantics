{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Semantics Model: Syntactic-Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/BASIC_TOPICS/DISTRIBUTIONAL_SEMANTICS/ASSIGNMENT_03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "stopwords = stopwords.words('english')\n",
    "from collections import defaultdict, Counter\n",
    "from functools import partial\n",
    "from itertools import permutations, product\n",
    "from string import punctuation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LoadWiki:\n",
    "    # load wiki dump in the following format:\n",
    "    # [ ([(rel, dependended, dependant), (..), ...], \n",
    "    #    [(tk, tk_lemma, pos), (..), ...]),\n",
    "    #   ([...], [...]), \n",
    "    #   ... ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.raw = []\n",
    "        with open('wikicorpus.txt','rb') as f:\n",
    "            bufferList = [] # temporarily store dependencies until encounter <c>.\n",
    "            for line in f.readlines():\n",
    "                if line.startswith('('):\n",
    "                    bufferList.append(self.__dependency_line_processor(line))\n",
    "                elif line.startswith('<c>'):\n",
    "                    self.raw.append((bufferList, self.__pos_line_processor(line)))\n",
    "                    bufferList = []\n",
    "                else: pass\n",
    "    \n",
    "    def __dependency_line_processor(self, line):\n",
    "        if line[-2]=='_': # get rid of parentheses in (rel,dpdd,dpdt,_) case.\n",
    "            tmp = line[1:-2].split()\n",
    "            return (tmp[0],tmp[1].split('_')[0],tmp[2].split('_')[0])\n",
    "        else: # get rid of parentheses in (rel,_,dpdd,dpdt), (rel,dpdd,dpdt), (rel,dpdd,dpdt,..), (..) case.\n",
    "            tmp = line[1:-1].split()\n",
    "            if len(tmp)<3: return\n",
    "            elif len(tmp)==3 or len(tmp)==4: return (tmp[0],tmp[1].split('_')[0],tmp[2].split('_')[0])\n",
    "            else: \n",
    "                print \"TEST: \", tmp\n",
    "                return (tmp[0],tmp[-2].split('_')[0],tmp[-1].split('_')[0]) # format: (rel, depended, dependant)\n",
    "    \n",
    "    def __term_splitter(self, term):\n",
    "        tmp = term.split('|')\n",
    "        return (tmp[0],tmp[1],tmp[2]) # save only tk, lm, pos in a tuple.\n",
    "    \n",
    "    def __pos_line_processor(self, line):\n",
    "        tmp = line[4:].split() # get rid of initial <c>\n",
    "        return map(self.__term_splitter, tmp) \n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 s, sys: 1.87 s, total: 33.9 s\n",
      "Wall time: 33.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = LoadWiki().get_data() # the number of sents = 397238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dependencies = [depList for depList,sentList in data]\n",
    "sents = [sentList for depList,sentList in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(w2w):\n",
    "    w2w_norm = w2w / np.apply_along_axis(lambda r: np.sqrt(np.dot(r,r))\n",
    "                               , 1, w2w)[:,np.newaxis]\n",
    "    return np.dot(w2w_norm, w2w_norm.T)\n",
    "    \n",
    "def ppmi(w2w):\n",
    "    rowSums, colSums, totalSums = w2w.sum(axis=1), w2w.sum(axis=0), w2w.sum()\n",
    "    pwi, pwj, ppmiMatrix = rowSums/totalSums, colSums/totalSums, w2w/totalSums\n",
    "    ppmiMatrix /= pwi[:,np.newaxis] # * 1/pwi by row.\n",
    "    ppmiMatrix /= pwj # * 1/pwj by col.\n",
    "    ppmiMatrix = np.nan_to_num(np.log(ppmiMatrix)) # compute pmi.\n",
    "    ppmiMatrix = np.maximum(ppmiMatrix, 0) # compute ppmi.\n",
    "    return ppmiMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DepedencyDict:\n",
    "    \n",
    "    def __init__(self, dependencies, sents):\n",
    "        self.dependencies = dependencies\n",
    "        self.sents = sents\n",
    "        self.depDict = defaultdict(list)\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.__build_dependency_dict()\n",
    "        \n",
    "    def __build_dependency_dict(self):\n",
    "        print \"... extracting dependencies\"\n",
    "        for i,sent in enumerate(sents):\n",
    "            for word in sent:\n",
    "                if word[2].startswith('N'):\n",
    "                    dps = self.__extract_dependencies(i,safe(word[0]))\n",
    "                    if len(dps)!=0: self.depDict[self.stemmer.stem(safe(word[0]))].extend(dps)\n",
    "    \n",
    "    def __extract_dependencies(self, sentIndex, word):\n",
    "        dps = []\n",
    "        for dp in self.dependencies[sentIndex]:\n",
    "            if dp is not None and word in dp:\n",
    "                dps.append(self.stemmer.stem(safe(dp[2]))) if word==safe(dp[1]) \\\n",
    "                    else dps.append(self.stemmer.stem(safe(dp[1])))\n",
    "            else: pass\n",
    "        return dps\n",
    "\n",
    "def safe(word):\n",
    "    return word.decode('utf-8','ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... extracting dependencies\n",
      "CPU times: user 1min 47s, sys: 457 ms, total: 1min 47s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "depDict = DepedencyDict(dependencies,sents).depDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleDistSem:\n",
    "    \n",
    "    def __init__(self, sents, depDict, kFrequent=50):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        print \"... counting word frequencies\"\n",
    "        self.freqCounts = Counter([self.stemmer.stem(safe(item[1])) for sent in sents for item in sent])\n",
    "        self.depDict = depDict\n",
    "        print \"... building vocabulary\"\n",
    "        self.vocab = [word for word in self.depDict.keys() if self.freqCounts[word]>kFrequent]\n",
    "        self.wordToIndex = {word:index for index,word in enumerate(self.vocab)}\n",
    "    \n",
    "    def build_w2w_matrix(self):\n",
    "        print \"... building w2w matrix\"\n",
    "        self.w2w = np.zeros((len(self.vocab),len(self.vocab)))\n",
    "        counter = 0\n",
    "        for word in self.vocab:\n",
    "            counter += 1\n",
    "            for dep in self.depDict[word]:\n",
    "                if dep in self.vocab:\n",
    "                    self.w2w[self.wordToIndex[word]][self.wordToIndex[dep]] += 1\n",
    "                    self.w2w[self.wordToIndex[dep]][self.wordToIndex[word]] += 1\n",
    "            if counter % 500 == 0: print \"... processed %d words\" % counter\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... counting word frequencies\n",
      "... building vocabulary\n",
      "CPU times: user 1min 22s, sys: 312 ms, total: 1min 22s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t = SimpleDistSem(sents,depDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building w2w matrix\n",
      "... processed 500 words\n",
      "... processed 1000 words\n",
      "... processed 1500 words\n",
      "... processed 2000 words\n",
      "... processed 2500 words\n",
      "... processed 3000 words\n",
      "... processed 3500 words\n",
      "... processed 4000 words\n",
      "... processed 4500 words\n",
      "... processed 5000 words\n",
      "... processed 5500 words\n",
      "... processed 6000 words\n",
      "... processed 6500 words\n",
      "... processed 7000 words\n",
      "... processed 7500 words\n",
      "... processed 8000 words\n",
      "... processed 8500 words\n",
      "CPU times: user 15min 36s, sys: 7.79 s, total: 15min 44s\n",
      "Wall time: 15min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t.build_w2w_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.9 s, sys: 645 ms, total: 40.6 s\n",
      "Wall time: 7.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cosineSimilarities = cosine(t.w2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.6 ms, sys: 1.47 ms, total: 19 ms\n",
      "Wall time: 18.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = t.vocab\n",
    "wordToIndex = t.wordToIndex\n",
    "indexToWord = {i:w for w,i in wordToIndex.iteritems()}\n",
    "words = ['car','bus','hospital','hotel','gun','bomb','horse','fox','table','bowl','guitar','piano']\n",
    "words = filter(lambda w:1 if w in vocab else 0, words)\n",
    "w2sim = {}\n",
    "for word in words:\n",
    "    simList = cosineSimilarities[wordToIndex[word]]\n",
    "    w2sim[word] = map(lambda idx:(indexToWord[idx],\n",
    "                                  cosineSimilarities[wordToIndex[word]][idx]),\n",
    "                      np.argsort(simList)[::-1][1:20+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'furthermor', nan),\n",
       " (u'Von', nan),\n",
       " (u'Abdul', nan),\n",
       " (u'Chrono', nan),\n",
       " (u'Kenneth', nan),\n",
       " (u'160', nan),\n",
       " (u'Tommi', nan),\n",
       " (u'comed', nan),\n",
       " (u'gun', 1.0000000000000004),\n",
       " (u'partner', 0.99651059833415512),\n",
       " (u'compon', 0.99633584457458257),\n",
       " (u'card', 0.99579196919351554),\n",
       " (u'processor', 0.99574415374759728),\n",
       " (u'rifl', 0.99554245509484351),\n",
       " (u'vehicl', 0.99550160533292054),\n",
       " (u'characterist', 0.99544846742783843),\n",
       " (u'regiment', 0.99543944807642926),\n",
       " (u'battalion', 0.99543862475385603),\n",
       " (u'engin', 0.99540763897557161),\n",
       " (u'facil', 0.99524023077397883)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2sim['gun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
