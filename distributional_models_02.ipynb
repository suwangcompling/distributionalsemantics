{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Semantics Model: Syntactic-Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/BASIC_TOPICS/DISTRIBUTIONAL_SEMANTICS/ASSIGNMENT_03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "stopwords = stopwords.words('english')\n",
    "from collections import defaultdict, Counter\n",
    "from functools import partial\n",
    "from itertools import permutations, product\n",
    "from string import punctuation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LoadWiki:\n",
    "    # load wiki dump in the following format:\n",
    "    # [ ([(rel, dependended, dependant), (..), ...], \n",
    "    #    [(tk, tk_lemma, pos), (..), ...]),\n",
    "    #   ([...], [...]), \n",
    "    #   ... ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.raw = []\n",
    "        with open('wikicorpus.txt','rb') as f:\n",
    "            bufferList = [] # temporarily store dependencies until encounter <c>.\n",
    "            for line in f.readlines():\n",
    "                if line.startswith('('):\n",
    "                    bufferList.append(self.__dependency_line_processor(line))\n",
    "                elif line.startswith('<c>'):\n",
    "                    self.raw.append((bufferList, self.__pos_line_processor(line)))\n",
    "                    bufferList = []\n",
    "                else: pass\n",
    "    \n",
    "    def __dependency_line_processor(self, line):\n",
    "        if line[-2]=='_': # get rid of parentheses in (rel,dpdd,dpdt,_) case.\n",
    "            tmp = line[1:-2].split()\n",
    "            return (tmp[0],tmp[1].split('_')[0],tmp[2].split('_')[0])\n",
    "        else: # get rid of parentheses in (rel,_,dpdd,dpdt), (rel,dpdd,dpdt), (rel,dpdd,dpdt,..), (..) case.\n",
    "            tmp = line[1:-1].split()\n",
    "            if len(tmp)<3: return\n",
    "            elif len(tmp)==3 or len(tmp)==4: return (tmp[0],tmp[1].split('_')[0],tmp[2].split('_')[0])\n",
    "            else: \n",
    "                return (tmp[0],tmp[-2].split('_')[0],tmp[-1].split('_')[0]) # format: (rel, depended, dependant)\n",
    "    \n",
    "    def __term_splitter(self, term):\n",
    "        tmp = term.split('|')\n",
    "        return (tmp[0],tmp[1],tmp[2]) # save only tk, lm, pos in a tuple.\n",
    "    \n",
    "    def __pos_line_processor(self, line):\n",
    "        tmp = line[4:].split() # get rid of initial <c>\n",
    "        return map(self.__term_splitter, tmp) \n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.4 s, sys: 1.99 s, total: 34.4 s\n",
      "Wall time: 34.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = LoadWiki().get_data() # the number of sents = 397238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dependencies = [depList for depList,sentList in data]\n",
    "sents = [sentList for depList,sentList in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "subVocab = [PorterStemmer().stem(word.decode('utf-8','ignore')) \n",
    "            for sent in sents \n",
    "            for word,_,pos in sent if pos.startswith('N')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(w2w):\n",
    "    w2w_norm = w2w / np.apply_along_axis(lambda r: np.sqrt(np.dot(r,r))\n",
    "                               , 1, w2w)[:,np.newaxis]\n",
    "    return np.dot(w2w_norm, w2w_norm.T)\n",
    "    \n",
    "def ppmi(w2w):\n",
    "    rowSums, colSums, totalSums = w2w.sum(axis=1), w2w.sum(axis=0), w2w.sum()\n",
    "    pwi, pwj, ppmiMatrix = rowSums/totalSums, colSums/totalSums, w2w/totalSums\n",
    "    ppmiMatrix /= pwi[:,np.newaxis] # * 1/pwi by row.\n",
    "    ppmiMatrix /= pwj # * 1/pwj by col.\n",
    "    ppmiMatrix = np.nan_to_num(np.log(ppmiMatrix)) # compute pmi.\n",
    "    ppmiMatrix = np.maximum(ppmiMatrix, 0) # compute ppmi.\n",
    "    return ppmiMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DepedencyDict:\n",
    "    \n",
    "    def __init__(self, dependencies, sents):\n",
    "        self.dependencies = dependencies\n",
    "        self.sents = sents\n",
    "        self.depDict = defaultdict(list)\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.__build_dependency_dict()\n",
    "        \n",
    "    def __build_dependency_dict(self):\n",
    "        print \"... extracting dependencies\"\n",
    "        for i,sent in enumerate(sents):\n",
    "            for word in sent:\n",
    "                if word[2].startswith('N'):\n",
    "                    dps = self.__extract_dependencies(i,safe(word[0]))\n",
    "                    if len(dps)!=0: self.depDict[self.stemmer.stem(safe(word[0]))].extend(dps)\n",
    "    \n",
    "    def __extract_dependencies(self, sentIndex, word):\n",
    "        dps = []\n",
    "        for dp in self.dependencies[sentIndex]:\n",
    "            if dp is not None and word in dp:\n",
    "                dps.append(self.stemmer.stem(safe(dp[2]))) if word==safe(dp[1]) \\\n",
    "                    else dps.append(self.stemmer.stem(safe(dp[1])))\n",
    "            else: pass\n",
    "        return dps\n",
    "\n",
    "def safe(word):\n",
    "    return word.decode('utf-8','ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... extracting dependencies\n",
      "CPU times: user 1min 54s, sys: 1.18 s, total: 1min 55s\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "depDict = DepedencyDict(dependencies,sents).depDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleDistSem:\n",
    "    \n",
    "    def __init__(self, sents, depDict, kFrequent=50):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        print \"... counting word frequencies\"\n",
    "        self.freqCounts = Counter([self.stemmer.stem(safe(word[0])) for sent in sents for word in sent])\n",
    "        self.depDict = depDict\n",
    "        print \"... building vocabulary\"\n",
    "        self.vocab = [word for word in self.depDict.keys() if self.freqCounts[word]>kFrequent]\n",
    "        self.contexts = list({value for values in ds.depDict.values() for value in values})\n",
    "        self.wordToIndex = {word:index for index,word in enumerate(self.vocab)}\n",
    "        self.contextToIndex = {context:index for index,context in enumerate(self.contexts)}\n",
    "    \n",
    "    def build_w2w_matrix(self):\n",
    "        \n",
    "        print \"... building full w2w matrix\"\n",
    "        self.w2c = np.zeros((len(self.vocab),len(self.contexts)))\n",
    "        counter = 0\n",
    "        for word in self.vocab:\n",
    "            counter += 1\n",
    "            for dep in self.depDict[word]:\n",
    "                self.w2c[self.wordToIndex[word]][self.contextToIndex[dep]] += 1\n",
    "            if counter % 500 == 0: print \"... processed %d words\" % counter\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... counting word frequencies\n",
      "... building vocabulary\n",
      "CPU times: user 1min 31s, sys: 408 ms, total: 1min 32s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = SimpleDistSem(sents,depDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building full w2w matrix\n",
      "... processed 500 words\n",
      "... processed 1000 words\n",
      "... processed 1500 words\n",
      "... processed 2000 words\n",
      "... processed 2500 words\n",
      "... processed 3000 words\n",
      "... processed 3500 words\n",
      "... processed 4000 words\n",
      "... processed 4500 words\n",
      "... processed 5000 words\n",
      "... processed 5500 words\n",
      "... processed 6000 words\n",
      "... processed 6500 words\n",
      "... processed 7000 words\n",
      "... processed 7500 words\n",
      "... processed 8000 words\n",
      "... processed 8500 words\n",
      "... processed 9000 words\n",
      "CPU times: user 4.78 s, sys: 418 ms, total: 5.2 s\n",
      "Wall time: 5.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_w2w_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 45s, sys: 1.19 s, total: 1min 46s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cosineSimilarities = cosine(ds.w2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.5 ms, sys: 2.19 ms, total: 20.7 ms\n",
      "Wall time: 20.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = ds.vocab\n",
    "wordToIndex = ds.wordToIndex\n",
    "indexToWord = {i:w for w,i in wordToIndex.iteritems()}\n",
    "words = ['car','bus','hospital','hotel','gun','bomb','horse','fox','table','bowl','guitar','piano']\n",
    "words = filter(lambda w:1 if w in vocab else 0, words)\n",
    "w2sim = {}\n",
    "for word in words:\n",
    "    simList = cosineSimilarities[wordToIndex[word]]\n",
    "    w2sim[word] = map(lambda idx:(indexToWord[idx],\n",
    "                                  cosineSimilarities[wordToIndex[word]][idx]),\n",
    "                      np.argsort(simList)[::-1][1:20+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ship', 0.99646414343514145),\n",
       " (u'machin', 0.99582312027210418),\n",
       " (u'design', 0.99581473404469589),\n",
       " (u'divis', 0.99553674599771103),\n",
       " (u'charact', 0.99535878392129906),\n",
       " (u'shell', 0.99523853078832369),\n",
       " (u'network', 0.99521343885009617),\n",
       " (u'structur', 0.99497686513167716),\n",
       " (u'class', 0.99496173147785805),\n",
       " (u'tank', 0.99479915008256747),\n",
       " (u'compani', 0.99477120855467938),\n",
       " (u'gener', 0.99471366053696786),\n",
       " (u'track', 0.99466696541411481),\n",
       " (u'build', 0.99465408211611483),\n",
       " (u'librari', 0.99450893332439605),\n",
       " (u'tube', 0.99446350552347296),\n",
       " (u'system', 0.99443211166413215),\n",
       " (u'celebr', 0.99435074029852422),\n",
       " (u'programm', 0.9943261932744496),\n",
       " (u'signal', 0.9943073166718277)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2sim['car']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def bless_evaluator(simMatrix=None, indexers=[None,None]):\n",
    "    wordToIndex, indexToWord = indexers\n",
    "    path = '/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/BASIC_TOPICS/DISTRIBUTIONAL_SEMANTICS/ASSIGNMENT_03/BLESS_part.txt'\n",
    "    with open(path,'rb') as f:\n",
    "        bless = f.readlines()\n",
    "    bless = [line.split('\\t') for line in bless] # split into (concept, _, relation, relatum).\n",
    "    \n",
    "    filter(lambda w:1 if w in vocab else 0, words)\n",
    "    \n",
    "    def filter_vocab(w):\n",
    "        if w in ds.vocab: return w # this is way too hacky, i know.\n",
    "    \n",
    "    crPairs = [(c.split('-')[0],r.split('-')[0],rel) for c,_,rel,r in bless]\n",
    "    posPairs = [(c,r) for c,r,rel in crPairs if rel=='hyper' \n",
    "                 if filter_vocab(c) is not None and filter_vocab(r) is not None]\n",
    "    negPairs = [(c,r) for c,r,rel in crPairs if rel=='mero' \n",
    "                 if filter_vocab(c) is not None and filter_vocab(r) is not None]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    return [map(lambda (c,r):(c,r,simMatrix[wordToIndex[stemmer.stem(c)]][wordToIndex[stemmer.stem(r)]]), posPairs),\n",
    "            map(lambda (c,r):(c,r,simMatrix[wordToIndex[stemmer.stem(c)]][wordToIndex[stemmer.stem(r)]]), negPairs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posEval, negEval = bless_evaluator(ds.w2c, indexers=[ds.wordToIndex, ds.vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Cosine): \n",
      "[('pig', 'mammal', 0.0), ('sword', 'object', 0.0), ('sword', 'artifact', 0.0), ('bull', 'mammal', 0.0), ('pistol', 'weapon', 0.0)]\n",
      "Average Cosine:  0.00558659217877\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(posEval, 5)\n",
    "print \"Average Cosine: \", np.mean([cosineVal for _,_,cosineVal in posEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Cosine): \n",
      "[('corn', 'kernel', 0.0), ('car', 'diesel', 0.0), ('car', 'light', 0.0), ('dolphin', 'tail', 0.0), ('pig', 'mouth', 0.0)]\n",
      "Average Cosine:  0.0218281036835\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(negEval, 5)\n",
    "print \"Average Cosine: \", np.mean([cosineVal for _,_,cosineVal in negEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
