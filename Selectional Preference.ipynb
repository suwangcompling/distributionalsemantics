{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnik (1996)\n",
    "\n",
    "* **Statistic**: Selectional Preference Strength\n",
    "* **Corpus**:\n",
    "    * Brown (NLTK)\n",
    "    * WordNet (NLTK)\n",
    "* **Parsing**:\n",
    "    * Type: Dependency\n",
    "    * Library: Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Main Function**: $S(p_i) = D_{KL}(Pr(c|p_i)||Pr(c)) = \\sum_cPr(c|p_i)log\\frac{Pr(c|p_i)}{Pr(c)}$ (cf. Resnik 1996:136)\n",
    "\n",
    "\n",
    "* **Probability of a Synset**: $Pr(c) = \\sum_{arg\\in c} Pr(arg)$\n",
    "\n",
    "\n",
    "* **Probability of a Synset Given Predicate**: $Pr(c|p_i) = \\sum_{arg\\in c} Pr(arg|p_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Synset Retriever (WordNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SynsetRetriever:\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def related_synsets(self, words):\n",
    "        \"\"\"\n",
    "        FUNCTION: \n",
    "            given a set of words, return all the synsets in which the word appears.\n",
    "        ARGS:\n",
    "            _words_: a list of words in strings.\n",
    "        \"\"\"\n",
    "        synsets = []\n",
    "        for word in words:\n",
    "            synsets += wn.synsets(word)\n",
    "        return list(set(synsets))\n",
    "    \n",
    "    def summary(self, word):\n",
    "        \"\"\"\n",
    "        FUNCTION:\n",
    "            given a word, prints: its synsets, pos + synset name + lemmas in synsets.\n",
    "        ARGS:\n",
    "            _word_: a word in string.\n",
    "        \"\"\"\n",
    "        synsets = wn.synsets(word)\n",
    "        print \"Synsets:\"\n",
    "        print\n",
    "        print synsets\n",
    "        print\n",
    "        print \"Pos & Names & Lemmas: \"\n",
    "        print\n",
    "        for synset in synsets:\n",
    "            print synset.pos(), '|', synset.name(), '|', synset.lemma_names()         \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr = SynsetRetriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synsets:\n",
      "\n",
      "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n",
      "\n",
      "Pos & Names & Lemmas: \n",
      "\n",
      "n | dog.n.01 | [u'dog', u'domestic_dog', u'Canis_familiaris']\n",
      "n | frump.n.01 | [u'frump', u'dog']\n",
      "n | dog.n.03 | [u'dog']\n",
      "n | cad.n.01 | [u'cad', u'bounder', u'blackguard', u'dog', u'hound', u'heel']\n",
      "n | frank.n.02 | [u'frank', u'frankfurter', u'hotdog', u'hot_dog', u'dog', u'wiener', u'wienerwurst', u'weenie']\n",
      "n | pawl.n.01 | [u'pawl', u'detent', u'click', u'dog']\n",
      "n | andiron.n.01 | [u'andiron', u'firedog', u'dog', u'dog-iron']\n",
      "v | chase.v.01 | [u'chase', u'chase_after', u'trail', u'tail', u'tag', u'give_chase', u'dog', u'go_after', u'track']\n"
     ]
    }
   ],
   "source": [
    "sr.summary('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Dependency Matrix (Brown + Spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from spacy.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dependency_matrix():\n",
    "    \"\"\"\n",
    "    FUNCTION:\n",
    "        computes a dependency matrix with: \n",
    "          - rows: predicates\n",
    "          - cols: arguments\n",
    "        returns \n",
    "          - argument set (for retrieving relevant synsets later)\n",
    "          - pred-to-index dictionary\n",
    "          - arg-to-index dictionary\n",
    "          - dependency matrix\n",
    "    \"\"\"\n",
    "    # extract info from corpus\n",
    "    sents = [' '.join(sent) for sent in brown.sents()]\n",
    "    tagged_words = brown.tagged_words(tagset='universal')\n",
    "    w2t = defaultdict(str,{w:t for w,t in tagged_words})\n",
    "    \n",
    "    # parser corpus\n",
    "    parser = English()\n",
    "    parsed_corpus = [parser(sent) for sent in sents]\n",
    "    \n",
    "    # find pred-arg pairs\n",
    "    pred_arg_pairs = [(token.head.orth_,token.orth_)\n",
    "                      for sent in parsed_corpus\n",
    "                      for token in sent\n",
    "                      if token.dep_=='dobj' and w2t[token.head.orth_]=='VERB']\n",
    "    arg_set = list({arg.lower() for pred,arg in pred_arg_pairs})\n",
    "    \n",
    "    # \"standardize\" preds and args by stemming\n",
    "    pred_arg_pairs = [(porter.stem(pred).lower(),porter.stem(arg)) \n",
    "                      for pred,arg in pred_arg_pairs]\n",
    "    \n",
    "    # build pred/arg-to-index dictionaries\n",
    "    pred_vocab = list({pred for pred,arg in pred_arg_pairs})\n",
    "    arg_vocab = list({arg for pred,arg in pred_arg_pairs})\n",
    "    pred2i = defaultdict(int, {p:i for i,p in enumerate(pred_vocab)})\n",
    "    arg2i = defaultdict(int, {a:i for i,a in enumerate(arg_vocab)})\n",
    "    \n",
    "    # build dependency matrix\n",
    "    dep_m = np.zeros((len(pred_vocab),len(arg_vocab)))\n",
    "    for pred,arg in pred_arg_pairs:\n",
    "        dep_m[pred2i[pred]][arg2i[arg]] += 1    \n",
    "    \n",
    "    return arg_set, pred2i, arg2i, dep_m \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min, sys: 2.54 s, total: 2min 2s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "arg_set, pred2i, arg2i, dep_m = dependency_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Selectional Preference Strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SPS:\n",
    "    \n",
    "    def __init__(self, arg_set, pred2i, arg2i, dep_m, synset_retriever):\n",
    "        \n",
    "        synsets = synset_retriever.related_synsets(arg_set)\n",
    "        total_freq = dep_m.sum()\n",
    "        porter = PorterStemmer()\n",
    "        \n",
    "        Pr_arg = lambda arg: sum(dep_m[:,arg2i[arg]]) / total_freq\n",
    "        Pr_c = lambda c: sum(Pr_arg(arg) for arg in get_words(c)) \n",
    "        get_words = lambda synset: [porter.stem(w) for w in synset.lemma_names()]\n",
    "        log = lambda x: np.log(x) if x!=0 else np.log(1e-20)\n",
    "        \n",
    "        def Pr_c_given_p(c,p):\n",
    "            args = get_words(c)\n",
    "            total_p_freq = sum(dep_m[pred2i[p],:])\n",
    "            return sum(dep_m[pred2i[p]][arg2i[arg]]/total_p_freq for arg in args)\n",
    "\n",
    "        self.sps = lambda p: sum(Pr_c_given_p(c,p)*log(Pr_c_given_p(c,p)/Pr_c(c)) \n",
    "                                 for c in synsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 231 ms, sys: 6.77 ms, total: 238 ms\n",
      "Wall time: 235 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sps = SPS(arg_set, pred2i, arg2i, dep_m, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.254626428061584"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sps.sps('push')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Mini Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **pour**, **drink** and **pack** are strongly selective words in terms of the set of direct object (Resnik 1996:138), whereas **give**, **make** and **have** are selectionally more promiscuous.\n",
    "\n",
    "* Therefore, the former words are expected to have higher SPS against the latter words.\n",
    "\n",
    "* **NB**: The numbers I got here are different from that in the original work, most likely due to use of updated corpus (Brown and WordNet), however the relative SPS of the words is solid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Selection Preference Strength\n",
      "\n",
      "'pour':  30.7441715022\n",
      "'drink':  26.5501401461\n",
      "'pack':  28.5965381572\n",
      "\n",
      "'give':  16.3344065669\n",
      "'make':  18.2618394795\n",
      "'have':  8.32228661623\n"
     ]
    }
   ],
   "source": [
    "print \"Selection Preference Strength\"\n",
    "print \n",
    "print \"'pour': \", sps.sps('pour')\n",
    "print \"'drink': \", sps.sps('drink')\n",
    "print \"'pack': \", sps.sps('pack')\n",
    "print\n",
    "print \"'give': \", sps.sps('give')\n",
    "print \"'make': \", sps.sps('make')\n",
    "print \"'have': \", sps.sps('have')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
