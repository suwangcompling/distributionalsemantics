{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Semantics Model: Syntactic-Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/BASIC_TOPICS/DISTRIBUTIONAL_SEMANTICS/ASSIGNMENT_03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "stopwords = stopwords.words('english')\n",
    "from collections import defaultdict, Counter\n",
    "from functools import partial\n",
    "from itertools import permutations, product\n",
    "from string import punctuation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LoadWiki:\n",
    "    # load wiki dump in the following format:\n",
    "    # [ ([(rel, dependended, dependant), (..), ...], \n",
    "    #    [(tk, tk_lemma, pos), (..), ...]),\n",
    "    #   ([...], [...]), \n",
    "    #   ... ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.raw = []\n",
    "        with open('wikicorpus.txt','rb') as f:\n",
    "            bufferList = [] # temporarily store dependencies until encounter <c>.\n",
    "            for line in f.readlines():\n",
    "                if line.startswith('('):\n",
    "                    bufferList.append(self.__dependency_line_processor(line))\n",
    "                elif line.startswith('<c>'):\n",
    "                    self.raw.append((bufferList, self.__pos_line_processor(line)))\n",
    "                    bufferList = []\n",
    "                else: pass\n",
    "    \n",
    "    def __dependency_line_processor(self, line):\n",
    "        if line[-2]=='_': # get rid of parentheses in (rel,dpdd,dpdt,_) case.\n",
    "            tmp = line[1:-2].split()\n",
    "            return (tmp[0],tmp[1].split('_')[0],tmp[2].split('_')[0])\n",
    "        else: # get rid of parentheses in (rel,_,dpdd,dpdt), (rel,dpdd,dpdt), (rel,dpdd,dpdt,..), (..) case.\n",
    "            tmp = line[1:-1].split()\n",
    "            if len(tmp)<3: return\n",
    "            elif len(tmp)==3 or len(tmp)==4: return (tmp[0],tmp[1].split('_')[0],tmp[2].split('_')[0])\n",
    "            else: \n",
    "                return (tmp[0],tmp[-2].split('_')[0],tmp[-1].split('_')[0]) # format: (rel, depended, dependant)\n",
    "    \n",
    "    def __term_splitter(self, term):\n",
    "        tmp = term.split('|')\n",
    "        return (tmp[0],tmp[1],tmp[2]) # save only tk, lm, pos in a tuple.\n",
    "    \n",
    "    def __pos_line_processor(self, line):\n",
    "        tmp = line[4:].split() # get rid of initial <c>\n",
    "        return map(self.__term_splitter, tmp) \n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34 s, sys: 2.31 s, total: 36.3 s\n",
      "Wall time: 36.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = LoadWiki().get_data() # the number of sents = 397238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dependencies = [depList for depList,sentList in data]\n",
    "sents = [sentList for depList,sentList in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# GET RID OF SENTS W/ EMPTY DEPS\n",
    "emptyIdx = [i for i,item in enumerate(dependencies) if item==[]]\n",
    "sents = [sent for i,sent in enumerate(sents) if i not in emptyIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dependencies = [dep for dep in dependencies if dep!=[]]\n",
    "    # len(deps)=len(sents)=370645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DEPENDENCY TYPES\n",
    "depTypes = list({dep[0] for depList in dependencies for dep in depList if dep is not None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xcomp', 'xmod', 'ncsubj', 'xsubj', 'ncmod', 'cmod', 'det', 'dobj', 'obj2', 'iobj', 'aux', 'conj', 'ccomp']\n"
     ]
    }
   ],
   "source": [
    "print depTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SAVE k MOST SYNTACTICALLY-SIGNIFICANT DISTINGUISHER RELS\n",
    "depTypesWide = ['xcomp','xmod','ncsubj','xsubj','ncmod','cmod','dobj','obj2','iobj','ccomp']\n",
    "depTypesNarrow = ['ncsubj','xsubj','ncmod','dobj','obj2','iobj']\n",
    "    # for meanings of tags, see http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.8742&rep=rep1&type=pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RESTRICT CONTEXT: ONLY FREQUENT-ENOUGH ONES\n",
    "tokens = [PorterStemmer().stem(safe(w)) for sent in sents for w,_,_ in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokensFreq = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'', 4779),\n",
       " (u'Y1l', 1),\n",
       " (u'Hansabank', 1),\n",
       " (u'Keach', 1),\n",
       " (u'woodi', 13),\n",
       " (u'un-studi', 1),\n",
       " (u'spideri', 1),\n",
       " (u'Simka', 1),\n",
       " (u'Pronk', 1),\n",
       " (u'Dagg', 1)]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokensFreq.items()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'nine', 485)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokensFreq.most_common(2001)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ncmod', '', 'anarchists'),\n",
       " ('ncmod', '', 'criteria'),\n",
       " ('dobj', 'constitutes', 'anarchism'),\n",
       " ('dobj', 'for', 'what'),\n",
       " ('ncmod', '', 'criteria'),\n",
       " ('dobj', 'have', 'criteria'),\n",
       " ('aux', 'have', 'may'),\n",
       " ('ncsubj', 'have', 'anarchists'),\n",
       " ('det', 'other', 'each'),\n",
       " ('dobj', 'with', 'other'),\n",
       " ('iobj', 'disagree', 'with'),\n",
       " ('det', 'criteria', 'these'),\n",
       " ('ncsubj', 'are', 'criteria'),\n",
       " ('dobj', 'on', 'what'),\n",
       " ('ncmod', '', 'disagree'),\n",
       " ('ncmod', '', 'disagree'),\n",
       " ('ncsubj', 'disagree', 'they'),\n",
       " ('conj', 'and', 'disagree'),\n",
       " ('conj', 'and', 'may')]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependencies[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Specific', 'Specific', 'NNP'),\n",
       " ('anarchists', 'anarchist', 'NNS'),\n",
       " ('may', 'may', 'MD'),\n",
       " ('have', 'have', 'VB'),\n",
       " ('additional', 'additional', 'JJ'),\n",
       " ('criteria', 'criterion', 'NNS'),\n",
       " ('for', 'for', 'IN'),\n",
       " ('what', 'what', 'WP'),\n",
       " ('constitutes', 'constitute', 'VBZ'),\n",
       " ('anarchism', 'anarchism', 'NN'),\n",
       " (',', ',', ','),\n",
       " ('and', 'and', 'CC'),\n",
       " ('they', 'they', 'PRP'),\n",
       " ('often', 'often', 'RB'),\n",
       " ('disagree', 'disagree', 'VBP'),\n",
       " ('with', 'with', 'IN'),\n",
       " ('each', 'each', 'DT'),\n",
       " ('other', 'other', 'NN'),\n",
       " ('on', 'on', 'IN'),\n",
       " ('what', 'what', 'WP'),\n",
       " ('these', 'these', 'DT'),\n",
       " ('criteria', 'criterion', 'NNS'),\n",
       " ('are', 'be', 'VBP'),\n",
       " ('.', '.', '.')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(w2w):\n",
    "    w2w_norm = w2w / np.apply_along_axis(lambda r: np.sqrt(np.dot(r,r))\n",
    "                               , 1, w2w)[:,np.newaxis]\n",
    "    return np.dot(w2w_norm, w2w_norm.T)\n",
    "\n",
    "# this PPMI algorithm unfit for w2w matrix when target words != context words!\n",
    "# def ppmi(w2w):\n",
    "#     rowSums, colSums, totalSums = w2w.sum(axis=1), w2w.sum(axis=0), w2w.sum()\n",
    "#     pwi, pwj, ppmiMatrix = rowSums/totalSums, colSums/totalSums, w2w/totalSums\n",
    "#     ppmiMatrix /= pwi[:,np.newaxis] # * 1/pwi by row.\n",
    "#     ppmiMatrix /= pwj # * 1/pwj by col.\n",
    "#     ppmiMatrix = np.nan_to_num(np.log(ppmiMatrix)) # compute pmi.\n",
    "#     ppmiMatrix = np.maximum(ppmiMatrix, 0) # compute ppmi.\n",
    "#     return ppmiMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DepedencyDict:\n",
    "    \n",
    "    def __init__(self, dependencies, sents):\n",
    "        self.dependencies = dependencies\n",
    "        self.sents = sents\n",
    "        self.depDict = defaultdict(list)\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.__build_dependency_dict()\n",
    "        \n",
    "    def __build_dependency_dict(self):\n",
    "        print \"... extracting dependencies\"\n",
    "        for i,sent in enumerate(sents):\n",
    "            for word in sent:\n",
    "                if word[2].startswith('N'):\n",
    "                    dps = self.__extract_dependencies(i,safe(word[0]))\n",
    "                    if len(dps)!=0: self.depDict[self.stemmer.stem(safe(word[0]))].extend(dps)\n",
    "    \n",
    "    def __extract_dependencies(self, sentIndex, word):\n",
    "        dps = []\n",
    "        for dp in self.dependencies[sentIndex]:\n",
    "            if dp is not None and word in dp and dp[0] in depTypesNarrow:\n",
    "                if word==safe(dp[1]) and dp[2] not in punctuation:\n",
    "                    dps.append(self.stemmer.stem(safe(dp[2])))\n",
    "                elif word==safe(dp[2]) and dp[1] not in punctuation:\n",
    "                    dps.append(self.stemmer.stem(safe(dp[1])))\n",
    "                else: pass\n",
    "            else: pass\n",
    "        return dps\n",
    "\n",
    "def safe(word):\n",
    "    return word.decode('utf-8','ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... extracting dependencies\n",
      "CPU times: user 1min 38s, sys: 1.07 s, total: 1min 39s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "depDict = DepedencyDict(dependencies,sents).depDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Specific', 'Specific', 'NNP'),\n",
       " ('anarchists', 'anarchist', 'NNS'),\n",
       " ('may', 'may', 'MD'),\n",
       " ('have', 'have', 'VB'),\n",
       " ('additional', 'additional', 'JJ'),\n",
       " ('criteria', 'criterion', 'NNS'),\n",
       " ('for', 'for', 'IN'),\n",
       " ('what', 'what', 'WP'),\n",
       " ('constitutes', 'constitute', 'VBZ'),\n",
       " ('anarchism', 'anarchism', 'NN'),\n",
       " (',', ',', ','),\n",
       " ('and', 'and', 'CC'),\n",
       " ('they', 'they', 'PRP'),\n",
       " ('often', 'often', 'RB'),\n",
       " ('disagree', 'disagree', 'VBP'),\n",
       " ('with', 'with', 'IN'),\n",
       " ('each', 'each', 'DT'),\n",
       " ('other', 'other', 'NN'),\n",
       " ('on', 'on', 'IN'),\n",
       " ('what', 'what', 'WP'),\n",
       " ('these', 'these', 'DT'),\n",
       " ('criteria', 'criterion', 'NNS'),\n",
       " ('are', 'be', 'VBP'),\n",
       " ('.', '.', '.')]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleDistSem:\n",
    "    \n",
    "    def __init__(self, sents, depDict, kFrequent=50):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        print \"... counting word frequencies\"\n",
    "        self.freqCounts = Counter([self.stemmer.stem(safe(word[0])) for sent in sents for word in sent])\n",
    "        self.depDict = depDict\n",
    "        print \"... building vocabulary\"\n",
    "        temp_vocab = [word for word in self.depDict.keys() if self.freqCounts[word]>kFrequent]\n",
    "        self.contexts = list({value for values in self.depDict.values() for value in values \n",
    "                              if tokensFreq[value]>400 and not ''})\n",
    "        self.vocab = [word for word in temp_vocab if any([context_word in self.contexts for context_word in self.depDict[word]])]\n",
    "        self.wordToIndex = {word:index for index,word in enumerate(self.vocab)}\n",
    "        self.contextToIndex = {context:index for index,context in enumerate(self.contexts)}\n",
    "    \n",
    "    def build_w2c_matrix(self):\n",
    "        \n",
    "        print \"... building full w2c matrix\"\n",
    "        self.w2c = np.zeros((len(self.vocab),len(self.contexts)))\n",
    "        counter = 0\n",
    "        for word in self.vocab:\n",
    "            counter += 1\n",
    "            for dep in self.depDict[word]:\n",
    "                if dep in self.contexts:\n",
    "                    self.w2c[self.wordToIndex[word]][self.contextToIndex[dep]] += 1\n",
    "            if counter % 500 == 0: print \"... processed %d words\" % counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'oldest'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.contexts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'upgrad' in ds.contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... counting word frequencies\n",
      "... building vocabulary\n"
     ]
    }
   ],
   "source": [
    "#%%timem\n",
    "ds = SimpleDistSem(sents,depDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building full w2c matrix\n",
      "... processed 500 words\n",
      "... processed 1000 words\n",
      "... processed 1500 words\n",
      "... processed 2000 words\n",
      "... processed 2500 words\n",
      "... processed 3000 words\n",
      "... processed 3500 words\n",
      "... processed 4000 words\n",
      "... processed 4500 words\n",
      "... processed 5000 words\n",
      "... processed 5500 words\n",
      "... processed 6000 words\n",
      "... processed 6500 words\n",
      "... processed 7000 words\n",
      "... processed 7500 words\n",
      "... processed 8000 words\n",
      "... processed 8500 words\n",
      "CPU times: user 2min 26s, sys: 1.97 s, total: 2min 28s\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_w2c_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8652, 2127)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.w2c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'',\n",
       " u'four',\n",
       " u'oldest',\n",
       " u'dynasti',\n",
       " u'whose',\n",
       " u'accus',\n",
       " u'accur',\n",
       " u'concret',\n",
       " u'Western',\n",
       " u'under']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.contexts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 590 ms, sys: 238 ms, total: 829 ms\n",
      "Wall time: 837 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ppmiSimilarities = ppmi(ds.w2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.7 ms, sys: 1.8 ms, total: 21.6 ms\n",
      "Wall time: 21.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = ds.vocab\n",
    "wordToIndex = ds.wordToIndex\n",
    "indexToWord = {i:w for w,i in wordToIndex.iteritems()}\n",
    "words = ['car','bus','hospital','hotel','gun','bomb','horse','fox','table','bowl','guitar','piano']\n",
    "words = filter(lambda w:1 if w in vocab else 0, words)\n",
    "w2sim = {}\n",
    "for word in words:\n",
    "    simList = ppmiSimilarities[wordToIndex[word]]\n",
    "    w2sim[word] = map(lambda idx:(indexToWord[idx],\n",
    "                                  ppmiSimilarities[wordToIndex[word]][idx]),\n",
    "                      np.argsort(simList)[::-1][1:20+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Era', 4.9201998645415781),\n",
       " (u'Crystal', 4.5708242230844576),\n",
       " (u'Altern', 4.4848817932837326),\n",
       " (u'altitud', 4.4003244052556694),\n",
       " (u'IEC', 4.3349416459928181),\n",
       " (u'Practic', 4.079416685175568),\n",
       " (u'Inca', 4.0222582713356196),\n",
       " (u'excav', 3.8899903027578691),\n",
       " (u'like', 3.8641471902922646),\n",
       " (u'Parson', 3.7113438973905701),\n",
       " (u'sexual', 3.7071772246957244),\n",
       " (u'cooper', 3.6805089776135631),\n",
       " (u'cadmium', 3.3541123929810914),\n",
       " (u'clergi', 3.3446635037831589),\n",
       " (u'Presley', 3.3291110907756742),\n",
       " (u'Energi', 3.310761952107478),\n",
       " (u'dilut', 3.2072212731666374),\n",
       " (u'Dick', 3.1327787969121212),\n",
       " (u'reput', 3.1322354662820748),\n",
       " (u'wherebi', 3.1284403953135231)]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2sim['car']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 s, sys: 301 ms, total: 11.2 s\n",
      "Wall time: 2.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cosineSimilarities = cosine(ds.w2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.3 ms, sys: 58.8 ms, total: 81.1 ms\n",
      "Wall time: 91.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = ds.vocab\n",
    "wordToIndex = ds.wordToIndex\n",
    "indexToWord = {i:w for w,i in wordToIndex.iteritems()}\n",
    "words = ['car','bus','hospital','hotel','gun','bomb','horse','fox','table','bowl','guitar','piano']\n",
    "words = filter(lambda w:1 if w in vocab else 0, words)\n",
    "w2sim = {}\n",
    "for word in words:\n",
    "    simList = cosineSimilarities[wordToIndex[word]]\n",
    "    w2sim[word] = map(lambda idx:(indexToWord[idx],\n",
    "                                  cosineSimilarities[wordToIndex[word]][idx]),\n",
    "                      np.argsort(simList)[::-1][1:20+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'engin', 0.94309930395038566),\n",
       " (u'system', 0.93603132778170495),\n",
       " (u'machin', 0.93453145228422441),\n",
       " (u'comput', 0.92688048974110959),\n",
       " (u'ship', 0.92525594823520196),\n",
       " (u'program', 0.92502353671012238),\n",
       " (u'vehicl', 0.92378263830994045),\n",
       " (u'oper', 0.92292229346271104),\n",
       " (u'design', 0.92091667909932851),\n",
       " (u'aircraft', 0.91975513877538151),\n",
       " (u'architectur', 0.91930301613423682),\n",
       " (u'train', 0.91903461634408234),\n",
       " (u'unit', 0.91724829715729406),\n",
       " (u'anim', 0.91670523078538657),\n",
       " (u'vessel', 0.91664412667338602),\n",
       " (u'divis', 0.9163243037919615),\n",
       " (u'activ', 0.91521280439143304),\n",
       " (u'tank', 0.91469086363275265),\n",
       " (u'experi', 0.91413547847980448),\n",
       " (u'boat', 0.91363809520929029)]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2sim['car']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### BLESS Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def bless_evaluator(simMatrix=None, indexers=[None,None]):\n",
    "    wordToIndex, indexToWord = indexers\n",
    "    path = '/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/BASIC_TOPICS/DISTRIBUTIONAL_SEMANTICS/ASSIGNMENT_03/BLESS_part.txt'\n",
    "    with open(path,'rb') as f:\n",
    "        bless = f.readlines()\n",
    "    bless = [line.split('\\t') for line in bless] # split into (concept, _, relation, relatum).\n",
    "    \n",
    "    filter(lambda w:1 if w in vocab else 0, words)\n",
    "    \n",
    "    def filter_vocab(w):\n",
    "        if w in ds.vocab: return w # this is way too hacky, i know.\n",
    "    \n",
    "    crPairs = [(c.split('-')[0],r.split('-')[0],rel) for c,_,rel,r in bless]\n",
    "    posPairs = [(c,r) for c,r,rel in crPairs if rel=='hyper' \n",
    "                 if filter_vocab(c) is not None and filter_vocab(r) is not None]\n",
    "    negPairs = [(c,r) for c,r,rel in crPairs if rel=='mero' \n",
    "                 if filter_vocab(c) is not None and filter_vocab(r) is not None]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    return [map(lambda (c,r):(c,r,simMatrix[wordToIndex[stemmer.stem(c)]][wordToIndex[stemmer.stem(r)]]), posPairs),\n",
    "            map(lambda (c,r):(c,r,simMatrix[wordToIndex[stemmer.stem(c)]][wordToIndex[stemmer.stem(r)]]), negPairs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posEval, negEval = bless_evaluator(cosineSimilarities, indexers=[ds.wordToIndex, ds.vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Cosine): \n",
      "[('potato', 'food', 0.59673120904353727), ('guitar', 'instrument', 0.85472223983043261), ('bull', 'beast', 0.62697262442409385), ('cat', 'mammal', 0.73782331602938822), ('rat', 'mammal', 0.91269136147598096)]\n",
      "Average Cosine:  0.685915096846\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(posEval, 5)\n",
    "print \"Average Cosine: \", np.mean([cosineVal for _,_,cosineVal in posEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Cosine): \n",
      "[('fork', 'plastic', 0.68949141703171235), ('van', 'floor', 0.19955151307702232), ('bomber', 'motor', 0.78723632322391224), ('cannon', 'ball', 0.7652596655938072), ('whale', 'fin', 0.34123184732844725)]\n",
      "Average Cosine:  0.596797240985\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(negEval, 5)\n",
    "print \"Average Cosine: \", np.mean([cosineVal for _,_,cosineVal in negEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
