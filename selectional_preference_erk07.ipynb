{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erk (2007)\n",
    "\n",
    "* **Statistic**: Selectional Preference Strength\n",
    "* **Corpus**: Brown\n",
    "* **Parsing**: \n",
    "    * Type: Dependency\n",
    "    * Library: Spacy\n",
    "* **Categorization**:\n",
    "    * Noun, Verb\n",
    "    * Noun Subject, Noun Direct Object, Verb\n",
    "\n",
    "**NB**: \n",
    "* This method seems to work better with large corpora, and less desirable on small ones compared to Resnik (1996).\n",
    "* For convenience, only one corpus is used here. In practice, it is recommended that a *primary corpus* and a *generalization corpus* are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Get Subj/Obj Lists for Verbs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_nv_info():\n",
    "    \"\"\"\n",
    "        returns an nv_count dict: {nsubj:[(n,v):count,...],dobj:[(n,v):count,...]}\n",
    "        returns a v-arg dict: {v:{nsubj:[...],dobj:[...]},...}\n",
    "    \"\"\"\n",
    "    \n",
    "    # build parsing facilities\n",
    "    parser = English()\n",
    "    nv_counts = {'nsubj':defaultdict(int),'dobj':defaultdict(int)}\n",
    "    dic = defaultdict(lambda : defaultdict(list))\n",
    "    def dictionarize(parsed):\n",
    "        for token in parsed:\n",
    "            if token.dep_ in ['nsubj','dobj'] \\\n",
    "                and w2t[token.head.orth_]=='VERB' \\\n",
    "                and w2t[token.orth_]=='NOUN':\n",
    "                nv_counts[token.dep_][(token.lemma_,token.head.lemma_)] += 1\n",
    "                if token.lemma_ not in dic[token.head.lemma_][token.dep_]:\n",
    "                    dic[token.head.lemma_][token.dep_].append(token.lemma_)\n",
    "                \n",
    "    # extract info from corpus\n",
    "    tagged_words = list(set(brown.tagged_words(tagset='universal')))\n",
    "    w2t = defaultdict(lambda : 'UNK', {w:t for w,t in tagged_words})\n",
    "    sents = [' '.join(sent) for sent in brown.sents()]\n",
    "    \n",
    "    # parse corpus\n",
    "    parsed_sents = [parser(sent) for sent in sents]\n",
    "    for parsed in parsed_sents:\n",
    "        dictionarize(parsed)\n",
    "    \n",
    "    return nv_counts, dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 46s, sys: 1.99 s, total: 1min 48s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nv_counts, dic = extract_nv_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Similarity Measure by Paradigmatic Co-Subj/Obj\n",
    "\n",
    "**NB**: This *similarity* is not the typical *distributional similarity*, in the sense that two words appear in similar contexts in general (e.g. some word-window). Instead, two words being similar here simply means they appear a lot as the subject/object of same verbs. *bread* and *boat* may be similar because they both appear as the object of the verb *sell*, but they are not similar in any intuitive sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Math**\n",
    "\n",
    "* **Association**: $PPMI(a,b) = \\begin{cases} PMI(a,b) \\quad &\\text{ if } \\geq 0 \\\\ 0 \\quad &\\text{ else } \\end{cases} $, where $PMI(a,b) = log\\frac{P(a,b)}{P(a)\\cdot P(b)}$\n",
    "\n",
    "\n",
    "* **Similarity**: $Cosine(u,v) = \\frac{\\sum_iu_iv_i}{\\sqrt{\\sum_iu_i^2}\\sqrt{\\sum_iv_i^2}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iszero(m): return m==0 # handling logging-on-0 or /0 issue.\n",
    "\n",
    "def ppmi(n2v):\n",
    "    row_sums, col_sums, total_sums = n2v.sum(axis=1), n2v.sum(axis=0), n2v.sum()\n",
    "    pn, pv, ppmi_m = row_sums/total_sums, col_sums/total_sums, n2v/total_sums\n",
    "    pn[iszero(pn)] = 1e-10 # NB: this is an expedient, may need to reconsider.\n",
    "    pv[iszero(pv)] = 1e-10\n",
    "    ppmi_m /= pn[:,np.newaxis] # * 1/pwi by row.\n",
    "    ppmi_m /= pv # * 1/pwj by col.\n",
    "    ppmi_m[iszero(ppmi_m)] = 1e-10 \n",
    "    ppmi_m = np.log(ppmi_m) # compute pmi.\n",
    "    ppmi_m = np.maximum(ppmi_m, 0) # compute ppmi.\n",
    "    return ppmi_m\n",
    "\n",
    "def cosine(n2v):\n",
    "    n2v_norm = n2v / np.apply_along_axis(lambda r: np.sqrt(np.dot(r,r)), 1, n2v)[:,np.newaxis]\n",
    "    return np.dot(n2v_norm, n2v_norm.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DistributionalModel:\n",
    "    \n",
    "    def __init__(self, nv_counts):\n",
    "        self.nvc = nv_counts\n",
    "        print \"... building vocabs\"\n",
    "        self._build_vocabs()\n",
    "        print \"... building n2v matrices\"\n",
    "        self._build_n2v_matrices()\n",
    "        print \"... building similarity matrices\"\n",
    "        self._build_similarity_matrices()\n",
    "        print \"READY!\"\n",
    "    \n",
    "    def _build_vocabs(self):\n",
    "        self.nsubj_vocab = list({n for n,v in self.nvc['nsubj'].iterkeys()})\n",
    "        self.dobj_vocab = list({n for n,v in self.nvc['dobj'].iterkeys()})\n",
    "        self.v_vocab = list({v for n,v in self.nvc['nsubj']}.union({v for n,v in self.nvc['dobj']}))\n",
    "        self.s2i = {s:i for i,s in enumerate(self.nsubj_vocab)}\n",
    "        self.o2i = {o:i for i,o in enumerate(self.dobj_vocab)}\n",
    "        self.v2i = {v:i for i,v in enumerate(self.v_vocab)}\n",
    "    \n",
    "    def _build_n2v_matrices(self):\n",
    "        self.nsubj_n2v = np.zeros((len(self.s2i),len(self.v2i)),dtype=float)\n",
    "        self.dobj_n2v = np.zeros((len(self.o2i),len(self.v2i)),dtype=float)\n",
    "        for (n,v),count in self.nvc['nsubj'].iteritems():\n",
    "            self.nsubj_n2v[self.s2i[n]][self.v2i[v]] = count\n",
    "        for (n,v),count in self.nvc['dobj'].iteritems():         \n",
    "            self.dobj_n2v[self.o2i[n]][self.v2i[v]] = count\n",
    "    \n",
    "    def _build_similarity_matrices(self):\n",
    "        self.nsubj_sim_m = cosine(ppmi(self.nsubj_n2v))\n",
    "        self.dobj_sim_m = cosine(ppmi(self.dobj_n2v))\n",
    "    \n",
    "    def nsubj_sim(self, n1, n2):\n",
    "        try: return self.nsubj_sim_m[self.s2i[n1]][self.s2i[n2]]\n",
    "        except: print \"Out of Vocabulary Words!\"\n",
    "    \n",
    "    def dobj_sim(self, n1, n2):\n",
    "        try: return self.dobj_sim_m[self.o2i[n1]][self.o2i[n2]]\n",
    "        except: print \"Out of Vocabulary Words!\"\n",
    "    \n",
    "    def nsubj_top_k(self, n, k=10, reverse=False):\n",
    "        try: n_idx = self.s2i[n]\n",
    "        except: \n",
    "            print \"Out of Vocabulary Words!\"\n",
    "            return\n",
    "        if reverse:\n",
    "            return map(lambda idx: self.nsubj_vocab[idx],\n",
    "                       np.argsort(self.nsubj_sim_m[n_idx])[:k])\n",
    "        return map(lambda idx: self.nsubj_vocab[idx],\n",
    "                   np.argsort(self.nsubj_sim_m[n_idx])[::-1][1:k+1])\n",
    "\n",
    "    def dobj_top_k(self, n, k=10, reverse=False):\n",
    "        try: n_idx = self.o2i[n]\n",
    "        except: \n",
    "            print \"Out of Vocabulary Words!\"\n",
    "            return\n",
    "        if reverse:\n",
    "            return map(lambda idx: self.dobj_vocab[idx],\n",
    "                       np.argsort(self.dobj_sim_m[n_idx])[:k])\n",
    "        return map(lambda idx: self.dobj_vocab[idx],\n",
    "                   np.argsort(self.dobj_sim_m[n_idx])[::-1][1:k+1])        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building vocabs\n",
      "... building n2v matrices\n",
      "... building similarity matrices\n",
      "READY!\n",
      "CPU times: user 16 s, sys: 867 ms, total: 16.8 s\n",
      "Wall time: 4.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sim = DistributionalModel(nv_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Compute Selectional Preference Strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Math**\n",
    "\n",
    "* **Selectional Preference Strength**: \n",
    "    * $S_v(w_0) = \\sum_{w\\in Seen(v)} SIM(w_0,w) \\cdot wt_v(w)$, where $Seen(v)$ is the set of seen nsubj/dobj of verb $v$.\n",
    "\n",
    "* **Weighting**:\n",
    "    * Uniform: $wt_v(w) = 1$\n",
    "    * By Frequency: $wt_v(w) = f(w,v)$\n",
    "    * By Discriminativity: $wt_v(w) = log\\frac{\\text{num. words}}{\\text{num. words to whose context w belongs}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weight Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n2v = {'s':sim.nsubj_n2v, 'o':sim.dobj_n2v} \n",
    "w2i = {'s':sim.s2i, 'o':sim.o2i, 'v':sim.v2i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log = lambda x: np.log(x) if x>0 else np.log(1e-20)\n",
    "div = lambda x,y: 0. if y==0 else x/y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wt(w, v, mode='s', wt_type='uniform'):\n",
    "    w_idx, v_idx = w2i[mode][w], w2i['v'][v]\n",
    "    if wt_type=='freq':\n",
    "        return n2v[mode][w_idx][v_idx]\n",
    "    elif wt_type=='disc':\n",
    "        num_w = sum(n2v[mode][w_idx,:])\n",
    "        num_context = sum(1 if cell!=0 else 0 for cell in n2v[mode][w_idx,:])\n",
    "        return log(div(num_w, num_context))\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selectional Preference Strength with Various Weightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_dic = {'s':sim.nsubj_sim, 'o':sim.dobj_sim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sps(w, v, mode='s', wt_type='uniform'):\n",
    "    \n",
    "    arg_vec = dic[v]['nsubj' if mode=='s' else 'dobj'] # dic is precomputed in section A.\n",
    "    return sum(sim_dic[mode](w,n) * wt(n,v,mode,wt_type) \n",
    "               for n in arg_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb 'eat''s Direct Object SPS\n",
      "\n",
      "eat-salad\n",
      "Uniform Wt.: 8.286084 | Freq. Wt.: 8.686217 | Disc. Wt.: 0.756887 \n",
      "\n",
      "eat-chicken\n",
      "Uniform Wt.: 5.264083 | Freq. Wt.: 6.379131 | Disc. Wt.: 0.643172 \n",
      "\n",
      "eat-jury\n",
      "Uniform Wt.: 0.042213 | Freq. Wt.: 0.042213 | Disc. Wt.: 0.031482 \n",
      "\n",
      "eat-court\n",
      "Uniform Wt.: 0.078737 | Freq. Wt.: 0.078737 | Disc. Wt.: 0.029188 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = ['salad','chicken','jury','court']\n",
    "verb = 'eat'\n",
    "print \"Verb 'eat''s Direct Object SPS\\n\"\n",
    "for word in words:\n",
    "    print \"%s-%s\\nUniform Wt.: %.6f | Freq. Wt.: %.6f | Disc. Wt.: %.6f \\n\" % \\\n",
    "          (verb,word,sps(word,verb,mode='o'),\n",
    "                     sps(word,verb,mode='o',wt_type='freq'),\n",
    "                     sps(word,verb,mode='o',wt_type='disc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
