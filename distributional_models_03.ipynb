{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Distributional Semantics Model: Narrow Word-Window (-2,+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data-Loading Facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/BASIC_TOPICS/DISTRIBUTIONAL_SEMANTICS/ASSIGNMENT_03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from itertools import permutations, product\n",
    "punctuation = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_wiki(cutoffFreq=50):\n",
    "    \n",
    "    print \"... extracting data\"\n",
    "    with open('wikicorpus.txt','rb') as f:\n",
    "        raw = f.readlines()\n",
    "    raw = [sent[4:].split() for sent in raw if sent.startswith('<c>')] \n",
    "        # sent[4:]: get rid of initial <c>.\n",
    "        # extract sentences; split sentences into word complexes.\n",
    "    raw = [[map(partial(str.split, word), '|') for word in sent] for sent in raw] \n",
    "        # split word complexes into words.\n",
    "    \n",
    "    print \"... cleaning data\"\n",
    "    sents = [[word[0][1].lower() for word in sent if len(word[0])>1 \n",
    "              and (word[0][2].startswith('N') or word[0][2]=='.')\n",
    "              and word[0][1].lower() not in stopwords\n",
    "              and word[0][1] not in punctuation] for sent in raw]\n",
    "        # extract lemmas => complete sents corpus .\n",
    "    \n",
    "    print \"... building token list and vocabulary\"\n",
    "    tokens = [word for sent in sents for word in sent]\n",
    "        # type: list of words.\n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    vocab = list(set(tokens))   \n",
    "    \n",
    "    print \"... saving top %d-frequent in vocabulary\" % cutoffFreq\n",
    "    vocab = [word for word in vocab if fdist[word] >= cutoffFreq]\n",
    "        # vocab is not returned, because the k-frequent cut latter can change it.\n",
    "    sents = [word.decode('utf-8','ignore') for sent in sents for word in sent if word in vocab]\n",
    "        # type: list of all words in the corpus, separated by periods.\n",
    "    sents.append('.') # handle -1 window lookahead +2 error.\n",
    "        \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... extracting data\n",
      "... cleaning data\n",
      "... building token list and vocabulary\n",
      "... saving top 50-frequent in vocabulary\n",
      "CPU times: user 4min 56s, sys: 7.46 s, total: 5min 3s\n",
      "Wall time: 5min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sents = load_wiki(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1. Analyzer: Cooccurrence Matrix Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SIMILARITY MEASURES\n",
    "def cosine(w2w):\n",
    "    w2w_norm = w2w / np.apply_along_axis(lambda r: np.sqrt(np.dot(r,r))\n",
    "                               , 1, w2w)[:,np.newaxis]\n",
    "    return np.dot(w2w_norm, w2w_norm.T)\n",
    "    \n",
    "def ppmi(w2w):\n",
    "    rowSums, colSums, totalSums = w2w.sum(axis=1), w2w.sum(axis=0), w2w.sum()\n",
    "    pwi, pwj, ppmiMatrix = rowSums/totalSums, colSums/totalSums, w2w/totalSums\n",
    "    ppmiMatrix /= pwi[:,np.newaxis] # * 1/pwi by row.\n",
    "    ppmiMatrix /= pwj # * 1/pwj by col.\n",
    "    ppmiMatrix = np.nan_to_num(np.log(ppmiMatrix)) # compute pmi.\n",
    "    ppmiMatrix = np.maximum(ppmiMatrix, 0) # compute ppmi.\n",
    "    return ppmiMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleDistSem:\n",
    "    \n",
    "    def __init__(self, data=sents, kFrequent=50):\n",
    "        self.sents = sents # a list of all words in the corpus (count=2573841).\n",
    "        self.vocab = list({word for word in sents})\n",
    "        self.wordToIndex = {word:i for i,word in enumerate(self.vocab)}\n",
    "        self.indexToWord = {i:word for word,i in self.wordToIndex.iteritems()}\n",
    "    \n",
    "    def build_w2w_matrix(self):\n",
    "        \n",
    "        print \"... counting words\"\n",
    "        cooccurrenceDict = defaultdict(int)\n",
    "        def add_entry(w_i,w_j): \n",
    "            cooccurrenceDict[(self.wordToIndex[w_i],self.wordToIndex[w_j])] += 1\n",
    "            cooccurrenceDict[(self.wordToIndex[w_j],self.wordToIndex[w_i])] += 1\n",
    "        for i,word in enumerate(self.sents):\n",
    "            if word=='.': continue # not pass, because we do not execute anything after this.\n",
    "            elif self.sents[i+2]=='.':\n",
    "                add_entry(word,self.sents[i-2])\n",
    "                add_entry(word,self.sents[i-1])\n",
    "                add_entry(word,self.sents[i+1])                \n",
    "            elif self.sents[i+1]=='.':\n",
    "                add_entry(word,self.sents[i-2])\n",
    "                add_entry(word,self.sents[i-1])\n",
    "            elif self.sents[i-1]=='.':\n",
    "                add_entry(word,self.sents[i+1])\n",
    "                add_entry(word,self.sents[i+2])\n",
    "            elif self.sents[i-2]=='.':\n",
    "                add_entry(word,self.sents[i-1])\n",
    "                add_entry(word,self.sents[i+1])\n",
    "                add_entry(word,self.sents[i+2])\n",
    "            elif i==0: \n",
    "                add_entry(word,self.sents[i+1])\n",
    "                add_entry(word,self.sents[i+2])\n",
    "            elif i==1: \n",
    "                add_entry(word,self.sents[i-1])\n",
    "                add_entry(word,self.sents[i+1])\n",
    "                add_entry(word,self.sents[i+2])\n",
    "            else: \n",
    "                add_entry(word,self.sents[i-2])\n",
    "                add_entry(word,self.sents[i-1])\n",
    "                add_entry(word,self.sents[i+1])\n",
    "                add_entry(word,self.sents[i+2])                \n",
    "        \n",
    "        print \"... building cooccurrence matrix\"\n",
    "        self.w2w = np.zeros((len(self.vocab),len(self.vocab)))\n",
    "        for (widx_i,widx_j),count in cooccurrenceDict.iteritems():\n",
    "            self.w2w[widx_i][widx_j] = count   \n",
    "    \n",
    "    def build_similarity_matrix(self, similarity=ppmi):\n",
    "        self.simMatrix = similarity(self.w2w)\n",
    "    \n",
    "    def k_most_similar(self, words, k=20):\n",
    "        assert len(words)==len(filter(lambda w:1 if w in self.vocab else 0, words))\n",
    "        w2sim = {}\n",
    "        for word in words:\n",
    "            simList = self.simMatrix[self.wordToIndex[word]]\n",
    "            w2sim[word] = map(lambda idx:(self.indexToWord[idx],\n",
    "                                          self.simMatrix[self.wordToIndex[word]][idx]),\n",
    "                              np.argsort(simList)[::-1][1:k+1])\n",
    "            # [1:k+1]: skip self-similarty.\n",
    "        return w2sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 298 ms, sys: 3.35 ms, total: 301 ms\n",
      "Wall time: 300 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = SimpleDistSem(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... counting words\n",
      "... building cooccurrence matrix\n",
      "CPU times: user 14.9 s, sys: 552 ms, total: 15.5 s\n",
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_w2w_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Evaluator: K-Frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.63 s, sys: 636 ms, total: 2.27 s\n",
      "Wall time: 2.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_similarity_matrix(ppmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.95 ms, sys: 985 µs, total: 9.93 ms\n",
      "Wall time: 8.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words = ['car','bus','hospital','hotel','gun','bomb','horse','fox','table','bowl','guitar','piano']\n",
    "w2sim = ds.k_most_similar(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'brabham', 4.7541390061810009),\n",
       " (u'bogie', 4.7196794382386171),\n",
       " (u'racing', 4.6159968217289284),\n",
       " (u'brake', 4.3700947885249093),\n",
       " (u'earnhardt', 4.3508297443719703),\n",
       " (u'bentley', 4.336193236849228),\n",
       " (u'cable', 4.333252451412112),\n",
       " (u'clutch', 4.3034034140262367),\n",
       " (u'audi', 4.1868158357746275),\n",
       " (u'chassis', 4.1559314130182834),\n",
       " (u'truck', 4.1102047334027869),\n",
       " (u'aston', 4.0998044587849973),\n",
       " (u'grip', 4.0076891698771915),\n",
       " (u'car', 3.9737637378896324),\n",
       " (u'prix', 3.9188333410889142),\n",
       " (u'installation', 3.8923217478803402),\n",
       " (u'motors', 3.8778311826189782),\n",
       " (u'tire', 3.8627233996270061),\n",
       " (u'luxury', 3.8419753494828979),\n",
       " (u'crash', 3.794477126063899)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2sim['car']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.6 s, sys: 310 ms, total: 22.9 s\n",
      "Wall time: 3.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_similarity_matrix(cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 ms, sys: 813 µs, total: 13.9 ms\n",
      "Wall time: 13.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words = ['car','bus','hospital','hotel','gun','bomb','horse','fox','table','bowl','guitar','piano']\n",
    "w2sim = ds.k_most_similar(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'concerto', 0.69962000806699731),\n",
       " (u'sonata', 0.66682645809820673),\n",
       " (u'cello', 0.66204078544049516),\n",
       " (u'violin', 0.62362338390182581),\n",
       " (u'trio', 0.54569317313435917),\n",
       " (u'bartk', 0.51422402079027529),\n",
       " (u'instrument', 0.49846288561417884),\n",
       " (u'bass', 0.49324167360193788),\n",
       " (u'composer', 0.48117823426359407),\n",
       " (u'guitar', 0.48099499031158183),\n",
       " (u'debussy', 0.47360253110598505),\n",
       " (u'quartet', 0.47347787243971523),\n",
       " (u'piece', 0.46483406817803208),\n",
       " (u'banjo', 0.44451716233948796),\n",
       " (u'repertoire', 0.42313460635578409),\n",
       " (u'solo', 0.4161858330271393),\n",
       " (u'string', 0.40897887998984539),\n",
       " (u'mozart', 0.40546351019303656),\n",
       " (u'music', 0.40448759314494348),\n",
       " (u'orchestra', 0.40356894488532824)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2sim['piano']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Evaluator: BLESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bless_evaluator(simMatrix=None, indexers=[None,None]):\n",
    "    wordToIndex, indexToWord = indexers\n",
    "    path = '/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/BASIC_TOPICS/DISTRIBUTIONAL_SEMANTICS/ASSIGNMENT_03/BLESS_part.txt'\n",
    "    with open(path,'rb') as f:\n",
    "        bless = f.readlines()\n",
    "    bless = [line.split('\\t') for line in bless] # split into (concept, _, relation, relatum).\n",
    "    crPairs = [(c.split('-')[0],r.split('-')[0],rel) for c,_,rel,r in bless]\n",
    "    posPairs = [(c,r) for c,r,rel in crPairs if rel=='hyper']\n",
    "    negPairs = [(c,r) for c,r,rel in crPairs if rel=='mero']\n",
    "    \n",
    "    return [map(lambda (c,r):(c,r,simMatrix[wordToIndex[c]][wordToIndex[r]]), posPairs),\n",
    "            map(lambda (c,r):(c,r,simMatrix[wordToIndex[c]][wordToIndex[r]]), negPairs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.7 s, sys: 625 ms, total: 2.33 s\n",
      "Wall time: 2.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_similarity_matrix(ppmi)\n",
    "posEval, negEval = bless_evaluator(ds.simMatrix, indexers=[ds.wordToIndex, ds.indexToWord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (PPMI): \n",
      "[('sheep', 'mammal', 2.9225073882803363), ('restaurant', 'construction', 1.6390546363558582), ('elephant', 'vertebrate', 0.0), ('pig', 'mammal', 0.0), ('goat', 'food', 1.3948544777760254)]\n",
      "Average PPMI:  1.50076975329\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (PPMI): \"\n",
    "print random.sample(posEval, 5)\n",
    "print \"Average PPMI: \", np.mean([ppmiVal for _,_,ppmiVal in posEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Negative Relations (PPMI): \n",
      "[('cathedral', 'archbishop', 3.3945832542402079), ('dress', 'neck', 0.0), ('truck', 'accelerator', 0.0), ('snake', 'skin', 3.2467953233848528), ('violin', 'neck', 0.0)]\n",
      "Average PPMI:  1.30109975988\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Negative Relations (PPMI): \"\n",
    "print random.sample(negEval, 5)\n",
    "print \"Average PPMI: \", np.mean([ppmiVal for _,_,ppmiVal in negEval])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.1 s, sys: 291 ms, total: 23.4 s\n",
      "Wall time: 3.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_similarity_matrix(cosine)\n",
    "posEval, negEval = bless_evaluator(ds.simMatrix, indexers=[ds.wordToIndex, ds.indexToWord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Cosine): \n",
      "[('cello', 'instrument', 0.60768635898244827), ('flute', 'instrument', 0.42458316313315286), ('piano', 'furniture', 0.22483378356353043), ('castle', 'housing', 0.18794690763427052), ('hospital', 'construction', 0.25102776063439169)]\n",
      "Average Cosine:  0.228813688324\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(posEval, 5)\n",
    "print \"Average Cosine: \", np.mean([cosineVal for _,_,cosineVal in posEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Cosine): \n",
      "[('train', 'engine', 0.19246061385996641), ('cathedral', 'ceiling', 0.19485368119580454), ('fighter', 'crew', 0.2569492415497649), ('lizard', 'leg', 0.22180841313329286), ('pistol', 'grip', 0.18631294795623843)]\n",
      "Average Cosine:  0.178549498886\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(negEval, 5)\n",
    "print \"Average Cosine: \", np.mean([cosineVal for _,_,cosineVal in negEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
