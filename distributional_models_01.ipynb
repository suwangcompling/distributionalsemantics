{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Semantics Model: Word-Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note: Similarity Measure **\n",
    "\n",
    "* **Cosine**: $ sim(u,v) = \\frac{\\sum_iu_iv_i}{\\sqrt{\\sum u_i^2}\\sqrt{\\sum v_i^2}} $\n",
    "\n",
    "\n",
    "* **PPMI**: $ sim(w_i,w_j) = max\\{log\\frac{P(w_i,w_j)}{P(w_i)\\cdot P(w_j)}, 0\\} $\n",
    "\n",
    "\n",
    "* **Jaccard**: $ sim(u,v) = \\frac{\\sum_imin\\{u_i,v_i\\}}{\\sum_imax\\{u_i,v_i\\}} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. \"Traditional\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data Loading Facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/BASIC_TOPICS/DISTRIBUTIONAL_SEMANTICS/ASSIGNMENT_03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from itertools import permutations, product\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_wiki(cutoffFreq=50):\n",
    "    \n",
    "    print \"... extracting data\"\n",
    "    with open('wikicorpus.txt','rb') as f:\n",
    "        raw = f.readlines()\n",
    "    raw = [sent.split() for sent in raw if sent.startswith('<c>')] \n",
    "        # extract sentences; split sentences into word complexes.\n",
    "    raw = [[map(partial(str.split, word), '|') for word in sent] for sent in raw] \n",
    "        # split word complexes into words.\n",
    "    \n",
    "    print \"... cleaning data\"\n",
    "    sents = [[word[0][1].lower() for word in sent if len(word[0])>1 \n",
    "              and word[0][2].startswith('N')\n",
    "              and word[0][1].lower() not in stopwords\n",
    "              and word[0][1] not in punctuation] for sent in raw]\n",
    "        # extract lemmas => complete sents corpus .\n",
    "    \n",
    "    print \"... building token list and vocabulary\"\n",
    "    tokens = [word for sent in sents for word in sent]\n",
    "        # type: list of words.\n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    vocab = list(set(tokens))   \n",
    "    \n",
    "    print \"... saving top %d-frequent in vocabulary\" % cutoffFreq\n",
    "    vocab = [word for word in vocab if fdist[word] >= cutoffFreq]\n",
    "        # vocab is not returned, because the k-frequent cut latter can change it.\n",
    "    sents = [[word.decode('utf-8','ignore') for word in sent if word in vocab] for sent in sents]\n",
    "        # type: list of lists of words.\n",
    "        \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... extracting data\n",
      "... cleaning data\n",
      "... building token list and vocabulary\n",
      "... saving top 50-frequent in vocabulary\n"
     ]
    }
   ],
   "source": [
    "sents = load_wiki()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1. Analyzer: Cooccurrence Matrix Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SIMILARITY MEASURES\n",
    "def cosine(w2w):\n",
    "    w2w_norm = w2w / np.apply_along_axis(lambda r: np.sqrt(np.dot(r,r))\n",
    "                               , 1, w2w)[:,np.newaxis]\n",
    "    return np.dot(w2w_norm, w2w_norm.T)\n",
    "    \n",
    "def ppmi(w2w):\n",
    "    rowSums, colSums, totalSums = w2w.sum(axis=1), w2w.sum(axis=0), w2w.sum()\n",
    "    pwi, pwj, ppmiMatrix = rowSums/totalSums, colSums/totalSums, w2w/totalSums\n",
    "    ppmiMatrix /= pwi[:,np.newaxis] # * 1/pwi by row.\n",
    "    ppmiMatrix /= pwj # * 1/pwj by col.\n",
    "    ppmiMatrix = np.nan_to_num(np.log(ppmiMatrix)) # compute pmi.\n",
    "    ppmiMatrix = np.maximum(ppmiMatrix, 0) # compute ppmi.\n",
    "    return ppmiMatrix\n",
    "\n",
    "def jaccard(w2w):\n",
    "    jaccardSimilarities = np.zeros(w2w.shape)\n",
    "    for i,wVec_i in enumerate(w2w):\n",
    "        for j,wVec_j in enumerate(w2w):\n",
    "            jaccardSimilarities[i][j] = sum(min(wVec_i,wVec_j) \n",
    "                                                for wVec_i,wVec_j in zip(wVec_i,wVec_j))/ \\\n",
    "                                        sum(max(wVec_i,wVec_j) \n",
    "                                                for wVec_i,wVec_j in zip(wVec_i,wVec_j))\n",
    "    return jaccardSimilarities\n",
    "    # SCIKIT-LEARN JACCARD\n",
    "    # >>> import numpy as np\n",
    "    # >>> from sklearn.metrics import jaccard_similarity_score\n",
    "    # >>> y_pred = [0, 2, 1, 3]\n",
    "    # >>> y_true = [0, 1, 2, 3]\n",
    "    # >>> jaccard_similarity_score(y_true, y_pred)\n",
    "    # 0.5\n",
    "    # >>> jaccard_similarity_score(y_true, y_pred, normalize=False)\n",
    "    # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleDistSem:\n",
    "    \n",
    "    def __init__(self, data=sents):\n",
    "        self.sents = sents\n",
    "        self.vocab = list({word for sent in self.sents for word in sent})\n",
    "        self.wordToIndex = {word:i for i,word in enumerate(self.vocab)}\n",
    "        self.indexToWord = {i:word for word,i in self.wordToIndex.iteritems()}\n",
    "    \n",
    "    def build_w2w_matrix(self):\n",
    "        \n",
    "        print \"... counting words\"\n",
    "        cooccurrenceDict = defaultdict(int)\n",
    "        def add_entry(word,contexts):\n",
    "            for context in contexts:\n",
    "                cooccurrenceDict[(self.wordToIndex[word],self.wordToIndex[context])] += 1\n",
    "        for sent in self.sents:\n",
    "            for word in sent:\n",
    "                contexts = [w for w in sent if w!=word]\n",
    "                add_entry(word,contexts)\n",
    "                \n",
    "        print \"... building cooccurrence matrix\"\n",
    "        self.w2w = np.zeros((len(self.vocab),len(self.vocab)))\n",
    "        for (widx_i,widx_j),count in cooccurrenceDict.iteritems():\n",
    "            self.w2w[widx_i][widx_j] = count\n",
    "    \n",
    "    def build_similarity_matrix(self, similarity=ppmi):\n",
    "        self.simMatrix = similarity(self.w2w)\n",
    "    \n",
    "    def k_most_similar(self, words, k=20):\n",
    "        assert len(words)==len(filter(lambda w:1 if w in self.vocab else 0, words))\n",
    "        w2sim = {}\n",
    "        for word in words:\n",
    "            simList = self.simMatrix[self.wordToIndex[word]]\n",
    "            w2sim[word] = map(lambda idx:(self.indexToWord[idx],\n",
    "                                          self.simMatrix[self.wordToIndex[word]][idx]),\n",
    "                              np.argsort(simList)[::-1][:k])\n",
    "            # [:k] -> [1:k+1] to skip self-similarty.\n",
    "        return w2sim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 414 ms, sys: 75 ms, total: 489 ms\n",
      "Wall time: 624 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = SimpleDistSem(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... counting words\n",
      "... building cooccurrence matrix\n",
      "CPU times: user 18.9 s, sys: 544 ms, total: 19.5 s\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_w2w_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Evaluator: K-Frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.76 s, sys: 677 ms, total: 2.44 s\n",
      "Wall time: 2.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_similarity_matrix(ppmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.81 ms, sys: 685 µs, total: 9.49 ms\n",
      "Wall time: 8.61 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words = ['car','bus','hospital','hotel','gun','bomb','horse','fox','table','bowl','guitar','piano']\n",
    "w2sim = ds.k_most_similar(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'bugatti', 4.9527736586389013),\n",
       " (u'brabham', 4.9161339514215019),\n",
       " (u'racing', 4.5414990646119815),\n",
       " (u'audi', 4.4569545520444169),\n",
       " (u'bogie', 4.4533767306965331),\n",
       " (u'clutch', 4.34957993701489),\n",
       " (u'brake', 4.303781867405136),\n",
       " (u'aston', 4.2316627721919184),\n",
       " (u'chassis', 4.2010965852956685),\n",
       " (u'grip', 4.1760155934775263),\n",
       " (u'earnhardt', 4.1697026796422909),\n",
       " (u'motors', 4.1104319795697029),\n",
       " (u'bentley', 4.1001287045704835),\n",
       " (u'cable', 4.000996428129918),\n",
       " (u'bmw', 3.9755909610087432),\n",
       " (u'prix', 3.8449707785916787),\n",
       " (u'driving', 3.8449707785916787),\n",
       " (u'tire', 3.8328002429714236),\n",
       " (u'drag', 3.7433277393339845),\n",
       " (u'wheel', 3.7327795828580266)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2sim['car']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.5 s, sys: 335 ms, total: 23.9 s\n",
      "Wall time: 4.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_similarity_matrix(cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 ms, sys: 1.54 ms, total: 14.6 ms\n",
      "Wall time: 12.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words = ['car','bus','hospital','hotel','gun','bomb','horse','fox','table','bowl','guitar','piano']\n",
    "w2sim = ds.k_most_similar(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'<text', nan),\n",
       " (u'piano', 0.99999999999999967),\n",
       " (u'composer', 0.69966561447368569),\n",
       " (u'cello', 0.65158806649998791),\n",
       " (u'concerto', 0.62814468385563815),\n",
       " (u'mozart', 0.61481959245104545),\n",
       " (u'violin', 0.60492309456281401),\n",
       " (u'repertoire', 0.5978334029835044),\n",
       " (u'bartk', 0.59402148569267632),\n",
       " (u'debussy', 0.59038513889274702),\n",
       " (u'bass', 0.58965311065244119),\n",
       " (u'quartet', 0.58928766235639241),\n",
       " (u'instrument', 0.57495837539565331),\n",
       " (u'orchestra', 0.57134920784866217),\n",
       " (u'trio', 0.56447162052240374),\n",
       " (u'sonata', 0.55335303018882542),\n",
       " (u'jazz', 0.54858777882388832),\n",
       " (u'vivaldi', 0.54782211463701003),\n",
       " (u'style', 0.54546613384641018),\n",
       " (u'ensemble', 0.53629183792580293)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2sim['piano']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ds.build_similarity_matrix(jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "words = ['car','bus','hospital','hotel','gun','bomb','horse','fox','table','bowl','guitar','piano']\n",
    "w2sim = ds.k_most_similar(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2sim['piano']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Evaluator: BLESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bless_evaluator(simMatrix=None, indexers=[None,None]):\n",
    "    wordToIndex, indexToWord = indexers\n",
    "    path = '/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/BASIC_TOPICS/DISTRIBUTIONAL_SEMANTICS/ASSIGNMENT_03/BLESS_part.txt'\n",
    "    with open(path,'rb') as f:\n",
    "        bless = f.readlines()\n",
    "    bless = [line.split('\\t') for line in bless] # split into (concept, _, relation, relatum).\n",
    "    crPairs = [(c.split('-')[0],r.split('-')[0],rel) for c,_,rel,r in bless]\n",
    "    posPairs = [(c,r) for c,r,rel in crPairs if rel=='hyper']\n",
    "    negPairs = [(c,r) for c,r,rel in crPairs if rel=='mero']\n",
    "    \n",
    "    return [map(lambda (c,r):(c,r,simMatrix[wordToIndex[c]][wordToIndex[r]]), posPairs),\n",
    "            map(lambda (c,r):(c,r,simMatrix[wordToIndex[c]][wordToIndex[r]]), negPairs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.64 s, sys: 637 ms, total: 2.27 s\n",
      "Wall time: 2.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_similarity_matrix(ppmi)\n",
    "posEval, negEval = bless_evaluator(ds.simMatrix, indexers=[ds.wordToIndex, ds.indexToWord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (PPMI): \n",
      "[('train', 'transport', 2.2302988178089156), ('car', 'transport', 1.1434975075691654), ('dress', 'clothing', 3.6998540116782288), ('piano', 'device', 0.54294334951781631), ('jet', 'craft', 0.0)]\n",
      "Average PPMI:  1.58804713869\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (PPMI): \"\n",
    "print random.sample(posEval, 5)\n",
    "print \"Average PPMI: \", np.mean([ppmiVal for _,_,ppmiVal in posEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Negative Relations (PPMI): \n",
      "[('van', 'belt', 2.3745710574629171), ('fighter', 'missile', 2.4072065468530175), ('car', 'brake', 4.303781867405136), ('horse', 'neck', 1.6025628271359271), ('coat', 'pattern', 2.1242004230418874)]\n",
      "Average PPMI:  1.37760253331\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Negative Relations (PPMI): \"\n",
    "print random.sample(negEval, 5)\n",
    "print \"Average PPMI: \", np.mean([ppmiVal for _,_,ppmiVal in negEval])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.4 s, sys: 282 ms, total: 23.7 s\n",
      "Wall time: 4.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_similarity_matrix(cosine)\n",
    "posEval, negEval = bless_evaluator(ds.simMatrix, indexers=[ds.wordToIndex, ds.indexToWord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Cosine): \n",
      "[('lizard', 'vertebrate', 0.30616568218162393), ('tiger', 'beast', 0.22822836358380735), ('bag', 'artifact', 0.18154098388172293), ('trumpet', 'device', 0.11180487394114597), ('beetle', 'insect', 0.36960661108537279)]\n",
      "Average Cosine:  0.295899824705\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(posEval, 5)\n",
    "print \"Average Cosine: \", np.mean([cosineVal for _,_,cosineVal in posEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Cosine): \n",
      "[('fork', 'tooth', 0.096313490741254823), ('pub', 'alcohol', 0.23298932669551869), ('cow', 'leg', 0.24205085152973466), ('pistol', 'barrel', 0.3866296621449728), ('hat', 'wool', 0.17767521564980449)]\n",
      "Average Cosine:  0.250902348547\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(negEval, 5)\n",
    "print \"Average Cosine: \", np.mean([cosineVal for _,_,cosineVal in negEval])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# ds.build_similarity_matrijaccardccard) # don't run this again if ran, super slow (1hr)\n",
    "posEval, negEval = bless_evaluator(ds.simMatrix, indexers=[ds.wordToIndex, ds.indexToWord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(posEval, 5)\n",
    "print \"Average Cosine: \", np.mean([jaccardVal for _,_,jaccardVal in posEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(negEval, 5)\n",
    "print \"Average Cosine: \", np.mean([jaccardVal for _,_,jaccardVal in negEval])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2. Analyzer: Tf-Idf Matrix Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import theano.tensor as T\n",
    "from theano import function\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cosine(tfidfMatrix):\n",
    "    # no difference from the previous cosine, only this is theano-based.\n",
    "    v = T.vector()\n",
    "    vLen = T.sqrt(T.dot(v,v))\n",
    "    vector_length = function([v], vLen)\n",
    "    M = T.matrix()\n",
    "    MtimesMT = T.dot(M,T.transpose(M))\n",
    "    multiply_matrix = function([M], MtimesMT)\n",
    "    tfidfMatrix_norm = tfidfMatrix / np.apply_along_axis(lambda r: vector_length(r).item(), \n",
    "                                                         1, tfidfMatrix)[:,np.newaxis]\n",
    "    return multiply_matrix(tfidfMatrix_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleDistSem:\n",
    "    \n",
    "    def __init__(self, data=load_wiki, kFrequent=50):\n",
    "        self.sents = load_wiki(kFrequent)\n",
    "   \n",
    "    def build_tfidf_matrix(self, dimension=100):\n",
    "        \n",
    "        print \"... building model\"\n",
    "        tfidf = TfidfVectorizer()\n",
    "        doc2vocab = tfidf.fit_transform([' '.join(sent) for sent in self.sents])\n",
    "            # this is a \"doc x vocab\" matrix (doc=sent in this case).\n",
    "        self.vocab = tfidf.vocabulary_.keys()\n",
    "        self.wordToIndex = tfidf.vocabulary_\n",
    "        self.indexToWord = {i:word for word,i in self.wordToIndex.iteritems()}\n",
    "        \n",
    "        print \"... building matrix\"\n",
    "        vocab2doc = doc2vocab.A.T\n",
    "            # this is a \"vocab x doc\" matrix.\n",
    "            \n",
    "        print \"... dimension reduction to %d\" % dimension\n",
    "        self.tfidfMatrix = TruncatedSVD(n_components=dimension).fit_transform(vocab2doc)        \n",
    "    \n",
    "    def build_similarity_matrix(self, similarity=cosine):\n",
    "        self.simMatrix = similarity(self.tfidfMatrix)\n",
    "    \n",
    "    def k_most_similar(self, words, k=20):\n",
    "        assert len(words)==len(filter(lambda w:1 if w in self.vocab else 0, words))\n",
    "        w2sim = {}\n",
    "        for word in words:\n",
    "            simList = self.simMatrix[self.wordToIndex[word]]\n",
    "            w2sim[word] = map(lambda idx:(self.indexToWord[idx],\n",
    "                                          self.simMatrix[self.wordToIndex[word]][idx]),\n",
    "                              np.argsort(simList)[::-1][:k])\n",
    "        return w2sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... extracting data\n",
      "... cleaning data\n",
      "... building token list and vocabulary\n",
      "... saving top 50-frequent in vocabulary\n",
      "CPU times: user 4min 51s, sys: 7.79 s, total: 4min 58s\n",
      "Wall time: 4min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = SimpleDistSem(kFrequent=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building model\n",
      "... building matrix\n",
      "... dimension reduction to 100\n",
      "CPU times: user 11min 29s, sys: 7min 16s, total: 18min 45s\n",
      "Wall time: 5min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_tfidf_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Evaluator: k-Frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 825 ms, sys: 133 ms, total: 958 ms\n",
      "Wall time: 743 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_similarity_matrix(cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 ms, sys: 551 µs, total: 11.2 ms\n",
      "Wall time: 10.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words = ['car','bus','hospital','hotel','gun','bomb','horse','fox','table','bowl','guitar','piano']\n",
    "w2sim = ds.k_most_similar(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'car', 1.0),\n",
       " (u'automobile', 0.78198512433256162),\n",
       " (u'manufacturer', 0.76825489012288861),\n",
       " (u'truck', 0.75936046819264114),\n",
       " (u'vehicle', 0.74034507695965179),\n",
       " (u'racing', 0.73296941278888672),\n",
       " (u'dodge', 0.73272329156215543),\n",
       " (u'ducati', 0.73025635105670872),\n",
       " (u'motorcycle', 0.72987873478148879),\n",
       " (u'aston', 0.72771677360033016),\n",
       " (u'factory', 0.7253843067022604),\n",
       " (u'motor', 0.7139360763061996),\n",
       " (u'bentley', 0.71382111106726009),\n",
       " (u'chrysler', 0.71184611174658552),\n",
       " (u'limited', 0.69660513896349774),\n",
       " (u'stock', 0.69489919650293464),\n",
       " (u'locomotive', 0.69143386648364602),\n",
       " (u'cable', 0.68722824072170041),\n",
       " (u'ford', 0.68420194003868762),\n",
       " (u'lease', 0.68215127135050091)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2sim['car']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Evaluator: BLESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bless_evaluator(simMatrix=None, indexers=[None,None]):\n",
    "    wordToIndex, indexToWord = indexers\n",
    "    path = '/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/BASIC_TOPICS/DISTRIBUTIONAL_SEMANTICS/ASSIGNMENT_03/BLESS_part.txt'\n",
    "    with open(path,'rb') as f:\n",
    "        bless = f.readlines()\n",
    "    bless = [line.split('\\t') for line in bless] # split into (concept, _, relation, relatum).\n",
    "    crPairs = [(c.split('-')[0],r.split('-')[0],rel) for c,_,rel,r in bless]\n",
    "    posPairs = [(c,r) for c,r,rel in crPairs if rel=='hyper']\n",
    "    negPairs = [(c,r) for c,r,rel in crPairs if rel=='mero']\n",
    "    \n",
    "    return [map(lambda (c,r):(c,r,simMatrix[wordToIndex[c]][wordToIndex[r]]), posPairs),\n",
    "            map(lambda (c,r):(c,r,simMatrix[wordToIndex[c]][wordToIndex[r]]), negPairs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 ms, sys: 4.95 ms, total: 18.8 ms\n",
      "Wall time: 19.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ds.build_similarity_matrix(cosine) # comment out if has been computed previously.\n",
    "posEval, negEval = bless_evaluator(ds.simMatrix, indexers=[ds.wordToIndex, ds.indexToWord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Cosine): \n",
      "[('turtle', 'pet', 0.45376598151239955), ('clarinet', 'artifact', 0.14487351849329463), ('battleship', 'ship', 0.75923317149541025), ('television', 'object', 0.066853681127950509), ('castle', 'building', 0.43025187595393954)]\n",
      "Average Cosine:  0.395534739769\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(posEval, 5)\n",
    "print \"Average Cosine: \", np.mean([cosineVal for _,_,cosineVal in posEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Cosine): \n",
      "[('van', 'gear', 0.41969753514314606), ('bus', 'plate', 0.11086664248226279), ('sword', 'point', -0.00087320182307122163), ('helicopter', 'wheel', 0.49287113499505791), ('car', 'door', 0.36863646621214197)]\n",
      "Average Cosine:  0.363030447208\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(negEval, 5)\n",
    "print \"Average Cosine: \", np.mean([cosineVal for _,_,cosineVal in negEval])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## II. Word Embedding Based (Gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... extracting data\n",
      "... cleaning data\n",
      "... building token list and vocabulary\n",
      "... saving top 50-frequent in vocabulary\n"
     ]
    }
   ],
   "source": [
    "sents = load_wiki()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'anarchism'],\n",
       " [u'anarchism', u'philosophy', u'theory', u'attitude', u'state'],\n",
       " [u'anarchist', u'criterion', u'anarchism', u'criterion'],\n",
       " [u'oxford',\n",
       "  u'companion',\n",
       "  u'philosophy',\n",
       "  u'position',\n",
       "  u'anarchist',\n",
       "  u'anarchist',\n",
       "  u'share',\n",
       "  u'family',\n",
       "  u'resemblance'],\n",
       " [u'type', u'tradition', u'anarchism']]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.8 s, sys: 1.29 s, total: 24.1 s\n",
      "Wall time: 9.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_model = Word2Vec(sents, size=100, window=4) \n",
    "    # if sents is raw data (i.e. low-freq. words unfiltered),\n",
    "    #  use 'min_count' to control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.75150329e-01,   6.99418545e-01,  -5.11950314e-01,\n",
       "         6.59707561e-02,   4.91376370e-01,  -4.77761716e-01,\n",
       "         5.42201400e-01,  -1.97092444e-01,   5.33426642e-01,\n",
       "        -3.37237179e-01,  -8.23528618e-02,  -4.95483190e-01,\n",
       "        -1.47832707e-01,  -5.31127512e-01,  -5.64234257e-01,\n",
       "         2.55174309e-01,  -6.66471720e-01,  -1.57762356e-02,\n",
       "         3.95696431e-01,   8.59119296e-02,   4.43060737e-04,\n",
       "        -5.26001334e-01,   2.14888588e-01,  -7.58717954e-02,\n",
       "        -4.19115603e-01,   1.17773570e-01,  -1.55215964e-01,\n",
       "         1.13844804e-01,   7.16124177e-01,  -6.85070634e-01,\n",
       "        -1.87814653e-01,   2.55785659e-02,  -2.51530170e-01,\n",
       "         6.57502770e-01,  -5.09228885e-01,  -1.33832723e-01,\n",
       "        -3.79972667e-01,  -7.30716437e-02,  -2.47103795e-02,\n",
       "        -1.50842983e-02,   1.57926455e-01,   1.72145128e-01,\n",
       "         2.03344733e-01,  -2.37459227e-01,  -7.21358359e-01,\n",
       "        -1.29057765e-01,   3.44870687e-01,   1.70839652e-01,\n",
       "        -9.33472216e-02,   2.95395434e-01,  -2.86266804e-01,\n",
       "        -1.50384992e-01,  -1.38800442e-01,   2.53089845e-01,\n",
       "         4.38569963e-01,   4.97456104e-01,   2.23515436e-01,\n",
       "         8.35578024e-01,  -3.95933360e-01,  -2.82037258e-01,\n",
       "         5.62950313e-01,  -1.29111767e-01,   3.25618327e-01,\n",
       "         3.67289670e-02,   1.50751835e-02,   1.55978248e-01,\n",
       "         6.91906884e-02,  -1.46125659e-01,   1.05792689e+00,\n",
       "        -7.90432811e-01,   2.42904216e-01,   2.09379643e-01,\n",
       "        -6.74290136e-02,  -7.01720268e-02,   7.94285893e-01,\n",
       "         6.09893203e-01,   3.87524426e-01,  -4.74823058e-01,\n",
       "         6.31066740e-01,  -7.34932795e-02,   5.35122931e-01,\n",
       "        -3.51929367e-01,   1.09171532e-01,   4.64342058e-01,\n",
       "         6.24376595e-01,   4.87322211e-02,   3.97764176e-01,\n",
       "        -3.67406197e-02,  -5.40095389e-01,   8.42574120e-01,\n",
       "         1.93939745e-01,  -3.58678073e-01,  -2.54659683e-01,\n",
       "        -4.52523008e-02,  -3.78452361e-01,   2.90974468e-01,\n",
       "         8.16459581e-03,   2.48441380e-02,   5.57997942e-01,\n",
       "         2.36781180e-01], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosineSimilarities = np.asarray([w2v_model[word] for word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = w2v_model.vocab.keys()\n",
    "wordToIndex = {word:i for i,word in enumerate(vocab)}\n",
    "indexToWord = {i:word for word,i in wordToIndex.iteritems()}\n",
    "w2vMatrix = np.asarray([w2v_model[word] for word in vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosineSimilarities = cosine(w2vMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Evaluator: K-Frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = ['car','bus','hospital','hotel','gun','bomb','horse','fox','table','bowl','guitar','piano']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert len(words)==len(filter(lambda w:1 if w in vocab else 0, words))\n",
    "w2sim = {}\n",
    "for word in words:\n",
    "    simList = cosineSimilarities[wordToIndex[word]]\n",
    "    w2sim[word] = map(lambda idx:(indexToWord[idx],\n",
    "                                  cosineSimilarities[wordToIndex[word]][idx]),\n",
    "                      np.argsort(simList)[::-1][1:20+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'racing', 0.80745568246577037),\n",
       " (u'motorcycle', 0.77827624676000107),\n",
       " (u'truck', 0.77548379907232889),\n",
       " (u'ducati', 0.76853153410659369),\n",
       " (u'driver', 0.76733914567819295),\n",
       " (u'vehicle', 0.75342257784875111),\n",
       " (u'automobile', 0.75091631398808334),\n",
       " (u'bmw', 0.75091198003567983),\n",
       " (u'audi', 0.74906836369815033),\n",
       " (u'bugatti', 0.74045194356275501),\n",
       " (u'brabham', 0.7319402436920569),\n",
       " (u'locomotive', 0.73175412390384009),\n",
       " (u'motor', 0.71965120134839877),\n",
       " (u'bogie', 0.71595481866357158),\n",
       " (u'bike', 0.7138517139922359),\n",
       " (u'taxi', 0.70554621519881633),\n",
       " (u'carriage', 0.70307595216451924),\n",
       " (u'bicycle', 0.69848066962656186),\n",
       " (u'rider', 0.69688658673939274),\n",
       " (u'competitor', 0.69254138733413384)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2sim['car']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Evaluator: BLESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posEval, negEval = bless_evaluator(cosineSimilarities, indexers=[wordToIndex, indexToWord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Cosine): \n",
      "[('guitar', 'artifact', 0.17882556861542048), ('deer', 'animal', 0.55486907926090812), ('turtle', 'creature', 0.59347283689587227), ('box', 'object', 0.099544941891068189), ('radio', 'artifact', -0.08771763428802383)]\n",
      "Average Cosine:  0.443507618235\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(posEval, 5)\n",
    "print \"Average Cosine: \", np.mean([cosineVal for _,_,cosineVal in posEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Cosine): \n",
      "[('castle', 'furniture', 0.27074505246656916), ('dress', 'button', 0.3428140659476267), ('truck', 'mirror', 0.29521023604557545), ('whale', 'mouth', 0.66658632206587454), ('cello', 'string', 0.81084896388357219)]\n",
      "Average Cosine:  0.426877469438\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(negEval, 5)\n",
    "print \"Average Cosine: \", np.mean([cosineVal for _,_,cosineVal in negEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
