{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Semantics Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note: Similarity Measure **\n",
    "\n",
    "* **Cosine**: $ sim(u,v) = \\frac{\\sum_iu_iv_i}{\\sqrt{\\sum u_i^2}\\sqrt{\\sum v_i^2}} $\n",
    "\n",
    "\n",
    "* **PPMI**: $ sim(w_i,w_j) = max\\{log\\frac{P(w_i,w_j)}{P(w_i)\\cdot P(w_j)}, 0\\} $\n",
    "\n",
    "\n",
    "* **Jaccard**: $ sim(u,v) = \\frac{\\sum_imin\\{u_i,v_i\\}}{\\sum_imax\\{u_i,v_i\\}} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. \"Traditional\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data Loading Facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/BASIC_TOPICS/DISTRIBUTIONAL_SEMANTICS/ASSIGNMENT_03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from itertools import permutations, product\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_wiki(cutoffFreq=50):\n",
    "    \n",
    "    print \"... extracting data\"\n",
    "    with open('wikicorpus.txt','rb') as f:\n",
    "        raw = f.readlines()\n",
    "    raw = [sent.split() for sent in raw if sent.startswith('<c>')] \n",
    "        # extract sentences; split sentences into word complexes.\n",
    "    raw = [[map(partial(str.split, word), '|') for word in sent] for sent in raw] \n",
    "        # split word complexes into words.\n",
    "    \n",
    "    print \"... cleaning data\"\n",
    "    sents = [[word[0][1].lower() for word in sent if len(word[0])>1 \n",
    "              and word[0][2].startswith('N')\n",
    "              and word[0][1].lower() not in stopwords\n",
    "              and word[0][1] not in punctuation] for sent in raw]\n",
    "        # extract lemmas => complete sents corpus .\n",
    "    \n",
    "    print \"... building token list and vocabulary\"\n",
    "    tokens = [word for sent in sents for word in sent]\n",
    "        # type: list of words.\n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    vocab = list(set(tokens))   \n",
    "    \n",
    "    print \"... saving top %d-frequent in vocabulary\" % cutoffFreq\n",
    "    vocab = [word for word in vocab if fdist[word] >= cutoffFreq]\n",
    "        # vocab is not returned, because the k-frequent cut latter can change it.\n",
    "    sents = [[word.decode('utf-8','ignore') for word in sent if word in vocab] for sent in sents]\n",
    "        # type: list of lists of words.\n",
    "        \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1. Analyzer: Cooccurrence Matrix Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SIMILARITY MEASURES\n",
    "def cosine(w2w):\n",
    "    w2w_norm = w2w / np.apply_along_axis(lambda r: np.sqrt(np.dot(r,r))\n",
    "                               , 1, w2w)[:,np.newaxis]\n",
    "    return np.dot(w2w_norm, w2w_norm.T)\n",
    "    \n",
    "def ppmi(w2w):\n",
    "    rowSums, colSums, totalSums = w2w.sum(axis=1), w2w.sum(axis=0), w2w.sum()\n",
    "    pwi, pwj, ppmiMatrix = rowSums/totalSums, colSums/totalSums, w2w/totalSums\n",
    "    ppmiMatrix /= pwi[:,np.newaxis] # * 1/pwi by row.\n",
    "    ppmiMatrix /= pwj # * 1/pwj by col.\n",
    "    ppmiMatrix = np.nan_to_num(np.log(ppmiMatrix)) # compute pmi.\n",
    "    ppmiMatrix = np.maximum(ppmiMatrix, 0) # compute ppmi.\n",
    "    return ppmiMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleDistSem:\n",
    "    \n",
    "    def __init__(self, data=load_wiki, kFrequent=50):\n",
    "        self.sents = load_wiki(kFrequent)\n",
    "        self.vocab = list({word for sent in self.sents for word in sent})\n",
    "        self.wordToIndex = {word:i for i,word in enumerate(self.vocab)}\n",
    "        self.indexToWord = {i:word for word,i in self.wordToIndex.iteritems()}\n",
    "    \n",
    "    def build_w2w_matrix(self):\n",
    "        \n",
    "        print \"... counting words\"\n",
    "        cooccurrenceDict = defaultdict(int)\n",
    "        for sent in self.sents:\n",
    "            for w_i,w_j in product(sent,repeat=2):\n",
    "                cooccurrenceDict[(self.wordToIndex[w_i],self.wordToIndex[w_j])] += 1\n",
    "                \n",
    "        print \"... building cooccurrence matrix\"\n",
    "        self.w2w = np.zeros((len(self.vocab),len(self.vocab)))\n",
    "        for (widx_i,widx_j),count in cooccurrenceDict.iteritems():\n",
    "            self.w2w[widx_i][widx_j] = count\n",
    "    \n",
    "    def build_similarity_matrix(self, similarity=ppmi):\n",
    "        self.simMatrix = similarity(self.w2w)\n",
    "    \n",
    "    def k_most_similar(self, words, k=20):\n",
    "        assert len(words)==len(filter(lambda w:1 if w in self.vocab else 0, words))\n",
    "        w2sim = {}\n",
    "        for word in words:\n",
    "            simList = self.simMatrix[self.wordToIndex[word]]\n",
    "            w2sim[word] = map(lambda idx:(self.indexToWord[idx],\n",
    "                                          self.simMatrix[self.wordToIndex[word]][idx]),\n",
    "                              np.argsort(simList)[::-1][:k])\n",
    "            # [:k] -> [1:k+1] to skip self-similarty.\n",
    "        return w2sim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... extracting data\n",
      "... cleaning data\n",
      "... building token list and vocabulary\n",
      "... saving top 50-frequent in vocabulary\n",
      "CPU times: user 4min 40s, sys: 6.35 s, total: 4min 47s\n",
      "Wall time: 4min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = SimpleDistSem(kFrequent=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... counting words\n",
      "... building cooccurrence matrix\n",
      "CPU times: user 16.3 s, sys: 433 ms, total: 16.7 s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_w2w_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Evaluator: K-Frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.65 s, sys: 559 ms, total: 2.21 s\n",
      "Wall time: 2.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_similarity_matrix(ppmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.8 ms, sys: 639 Âµs, total: 9.44 ms\n",
      "Wall time: 9.43 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words = ['car','bus','hospital','hotel','gun','bomb','horse','fox','table','bowl','guitar','piano']\n",
    "w2sim = ds.k_most_similar(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'car', 5.5676816083512408),\n",
       " (u'brabham', 4.7370186924636686),\n",
       " (u'bugatti', 4.7237837775965463),\n",
       " (u'racing', 4.3543655930385992),\n",
       " (u'bogie', 4.1806549399146231),\n",
       " (u'audi', 4.1720831279024368),\n",
       " (u'brake', 4.110064293137321),\n",
       " (u'clutch', 4.0859371771892361),\n",
       " (u'aston', 4.0789773262541758),\n",
       " (u'chassis', 4.0359974554728177),\n",
       " (u'grip', 3.9920710409571445),\n",
       " (u'motors', 3.9754986673044659),\n",
       " (u'earnhardt', 3.9556109155136614),\n",
       " (u'bentley', 3.8900626539905776),\n",
       " (u'cable', 3.8163895589909154),\n",
       " (u'bmw', 3.7358020900909055),\n",
       " (u'prix', 3.6938950894132119),\n",
       " (u'driving', 3.6889025678100911),\n",
       " (u'tire', 3.6707417844384955),\n",
       " (u'drag', 3.5695512828426086)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2sim['car']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.2 s, sys: 257 ms, total: 20.4 s\n",
      "Wall time: 3.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_similarity_matrix(cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 620 Âµs, total: 12.7 ms\n",
      "Wall time: 11.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words = ['car','bus','hospital','hotel','gun','bomb','horse','fox','table','bowl','guitar','piano']\n",
    "w2sim = ds.k_most_similar(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'piano', 1.0),\n",
       " (u'sonata', 0.45607377928410825),\n",
       " (u'trio', 0.35838892239353981),\n",
       " (u'concerto', 0.32427804909836766),\n",
       " (u'pianist', 0.3046071109326966),\n",
       " (u'violin', 0.3016548704228233),\n",
       " (u'bartk', 0.28394365707596747),\n",
       " (u'beethoven', 0.2642449285886056),\n",
       " (u'cello', 0.2330752071904453),\n",
       " (u'mozart', 0.2111765802790872),\n",
       " (u'music', 0.20778712041515557),\n",
       " (u'quartet', 0.19455367242470653),\n",
       " (u'guitar', 0.18879834891146863),\n",
       " (u'satie', 0.18168601655873329),\n",
       " (u'flute', 0.18126266038699335),\n",
       " (u'composer', 0.17935882766467443),\n",
       " (u'shostakovich', 0.17870131920452129),\n",
       " (u'debussy', 0.17265077192418996),\n",
       " (u'bass', 0.17134824877272578),\n",
       " (u'percussion', 0.16699620460987527)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2sim['piano']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Evaluator: BLESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bless_evaluator(simMatrix=None, indexers=[None,None]):\n",
    "    wordToIndex, indexToWord = indexers\n",
    "    path = '/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/BASIC_TOPICS/DISTRIBUTIONAL_SEMANTICS/ASSIGNMENT_03/BLESS_part.txt'\n",
    "    with open(path,'rb') as f:\n",
    "        bless = f.readlines()\n",
    "    bless = [line.split('\\t') for line in bless] # split into (concept, _, relation, relatum).\n",
    "    crPairs = [(c.split('-')[0],r.split('-')[0],rel) for c,_,rel,r in bless]\n",
    "    posPairs = [(c,r) for c,r,rel in crPairs if rel=='hyper']\n",
    "    negPairs = [(c,r) for c,r,rel in crPairs if rel=='mero']\n",
    "    \n",
    "    return [map(lambda (c,r):(c,r,simMatrix[wordToIndex[c]][wordToIndex[r]]), posPairs),\n",
    "            map(lambda (c,r):(c,r,simMatrix[wordToIndex[c]][wordToIndex[r]]), negPairs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.65 s, sys: 551 ms, total: 2.2 s\n",
      "Wall time: 2.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_similarity_matrix(ppmi)\n",
    "posEval, negEval = bless_evaluator(ds.simMatrix, indexers=[ds.wordToIndex, ds.indexToWord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (PPMI): \n",
      "[('hammer', 'artifact', 0.0), ('goat', 'food', 1.9087405887647915), ('horse', 'creature', 1.2505876398833882), ('fox', 'beast', 2.852315961784635), ('fighter', 'vehicle', 1.3481053885271306)]\n",
      "Average PPMI:  1.48619039202\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (PPMI): \"\n",
    "print random.sample(posEval, 5)\n",
    "print \"Average PPMI: \", np.mean([ppmiVal for _,_,ppmiVal in posEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Negative Relations (PPMI): \n",
      "[('helicopter', 'tail', 2.7288466816579824), ('cannon', 'ammunition', 4.3424453266452474), ('motorcycle', 'tank', 0.0), ('hammer', 'steel', 1.9859433074758956), ('hospital', 'window', 0.0)]\n",
      "Average PPMI:  1.27640054163\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Negative Relations (PPMI): \"\n",
    "print random.sample(negEval, 5)\n",
    "print \"Average PPMI: \", np.mean([ppmiVal for _,_,ppmiVal in negEval])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.2 s, sys: 295 ms, total: 20.5 s\n",
      "Wall time: 3.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_similarity_matrix(cosine)\n",
    "posEval, negEval = bless_evaluator(ds.simMatrix, indexers=[ds.wordToIndex, ds.indexToWord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Cosine): \n",
      "[('motorcycle', 'transport', 0.029442142376491426), ('hotel', 'structure', 0.017216513144524249), ('television', 'equipment', 0.058559154251075898), ('coyote', 'creature', 0.019324295262112278), ('turtle', 'vertebrate', 0.037552685932305083)]\n",
      "Average Cosine:  0.0603423768913\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(posEval, 5)\n",
    "print \"Average Cosine: \", np.mean([cosineVal for _,_,cosineVal in posEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Cosine): \n",
      "[('cello', 'back', 0.053753511464451946), ('snake', 'poison', 0.033404743658572426), ('pub', 'glass', 0.022851286650711126), ('salmon', 'mouth', 0.061380927946826883), ('hotel', 'window', 0.025526976678457877)]\n",
      "Average Cosine:  0.0468720110053\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(negEval, 5)\n",
    "print \"Average Cosine: \", np.mean([cosineVal for _,_,cosineVal in negEval])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2. Analyzer: Tf-Idf Matrix Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import theano.tensor as T\n",
    "from theano import function\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cosine(tfidfMatrix):\n",
    "    # no difference from the previous cosine, only this is theano-based.\n",
    "    v = T.vector()\n",
    "    vLen = T.sqrt(T.dot(v,v))\n",
    "    vector_length = function([v], vLen)\n",
    "    M = T.matrix()\n",
    "    MtimesMT = T.dot(M,T.transpose(M))\n",
    "    multiply_matrix = function([M], MtimesMT)\n",
    "    tfidfMatrix_norm = tfidfMatrix / np.apply_along_axis(lambda r: vector_length(r).item(), \n",
    "                                                         1, tfidfMatrix)[:,np.newaxis]\n",
    "    return multiply_matrix(tfidfMatrix_norm)\n",
    "\n",
    "def jaccard(tfidfMatrix):\n",
    "    jaccardSimilarities = np.zeros((tfidfMatrix.shape[0],tfidfMatrix.shape[0]))\n",
    "    for i,wVec_i in enumerate(tfidfMatrix):\n",
    "        for j,wVec_j in enumerate(tfidfMatrix):\n",
    "            jaccardSimilarities[i][j] = sum(min(wVec_i,wVec_j) \n",
    "                                                for wVec_i,wVec_j in zip(wVec_i,wVec_j))/ \\\n",
    "                                        sum(max(wVec_i,wVec_j) \n",
    "                                                for wVec_i,wVec_j in zip(wVec_i,wVec_j))\n",
    "    return jaccardSimilarities\n",
    "\n",
    "# SCIKIT-LEARN JACCARD\n",
    "# >>> import numpy as np\n",
    "# >>> from sklearn.metrics import jaccard_similarity_score\n",
    "# >>> y_pred = [0, 2, 1, 3]\n",
    "# >>> y_true = [0, 1, 2, 3]\n",
    "# >>> jaccard_similarity_score(y_true, y_pred)\n",
    "# 0.5\n",
    "# >>> jaccard_similarity_score(y_true, y_pred, normalize=False)\n",
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleDistSem:\n",
    "    \n",
    "    def __init__(self, data=load_wiki, kFrequent=50):\n",
    "        self.sents = load_wiki(kFrequent)\n",
    "   \n",
    "    def build_tfidf_matrix(self, dimension=100):\n",
    "        \n",
    "        print \"... building model\"\n",
    "        tfidf = TfidfVectorizer()\n",
    "        doc2vocab = tfidf.fit_transform([' '.join(sent) for sent in self.sents])\n",
    "            # this is a \"doc x vocab\" matrix (doc=sent in this case).\n",
    "        self.vocab = tfidf.vocabulary_.keys()\n",
    "        self.wordToIndex = tfidf.vocabulary_\n",
    "        self.indexToWord = {i:word for word,i in self.wordToIndex.iteritems()}\n",
    "        \n",
    "        print \"... building matrix\"\n",
    "        vocab2doc = doc2vocab.A.T\n",
    "            # this is a \"vocab x doc\" matrix.\n",
    "            \n",
    "        print \"... dimension reduction to %d\" % dimension\n",
    "        self.tfidfMatrix = TruncatedSVD(n_components=dimension).fit_transform(vocab2doc)        \n",
    "    \n",
    "    def build_similarity_matrix(self, similarity=cosine):\n",
    "        self.simMatrix = similarity(self.tfidfMatrix)\n",
    "    \n",
    "    def k_most_similar(self, words, k=20):\n",
    "        assert len(words)==len(filter(lambda w:1 if w in self.vocab else 0, words))\n",
    "        w2sim = {}\n",
    "        for word in words:\n",
    "            simList = self.simMatrix[self.wordToIndex[word]]\n",
    "            w2sim[word] = map(lambda idx:(self.indexToWord[idx],\n",
    "                                          self.simMatrix[self.wordToIndex[word]][idx]),\n",
    "                              np.argsort(simList)[::-1][:k])\n",
    "        return w2sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... extracting data\n",
      "... cleaning data\n",
      "... building token list and vocabulary\n",
      "... saving top 50-frequent in vocabulary\n",
      "CPU times: user 4min 51s, sys: 7.79 s, total: 4min 58s\n",
      "Wall time: 4min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = SimpleDistSem(kFrequent=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building model\n",
      "... building matrix\n",
      "... dimension reduction to 100\n",
      "CPU times: user 11min 29s, sys: 7min 16s, total: 18min 45s\n",
      "Wall time: 5min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_tfidf_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Evaluator: k-Frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 825 ms, sys: 133 ms, total: 958 ms\n",
      "Wall time: 743 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_similarity_matrix(cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 ms, sys: 551 Âµs, total: 11.2 ms\n",
      "Wall time: 10.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words = ['car','bus','hospital','hotel','gun','bomb','horse','fox','table','bowl','guitar','piano']\n",
    "w2sim = ds.k_most_similar(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'car', 1.0),\n",
       " (u'automobile', 0.78198512433256162),\n",
       " (u'manufacturer', 0.76825489012288861),\n",
       " (u'truck', 0.75936046819264114),\n",
       " (u'vehicle', 0.74034507695965179),\n",
       " (u'racing', 0.73296941278888672),\n",
       " (u'dodge', 0.73272329156215543),\n",
       " (u'ducati', 0.73025635105670872),\n",
       " (u'motorcycle', 0.72987873478148879),\n",
       " (u'aston', 0.72771677360033016),\n",
       " (u'factory', 0.7253843067022604),\n",
       " (u'motor', 0.7139360763061996),\n",
       " (u'bentley', 0.71382111106726009),\n",
       " (u'chrysler', 0.71184611174658552),\n",
       " (u'limited', 0.69660513896349774),\n",
       " (u'stock', 0.69489919650293464),\n",
       " (u'locomotive', 0.69143386648364602),\n",
       " (u'cable', 0.68722824072170041),\n",
       " (u'ford', 0.68420194003868762),\n",
       " (u'lease', 0.68215127135050091)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2sim['car']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 58s, sys: 42.9 s, total: 1h 1min 41s\n",
      "Wall time: 1h 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds.build_similarity_matrix(jaccard) # super slow, searching for better algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.2 ms, sys: 578 Âµs, total: 10.8 ms\n",
      "Wall time: 10 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words = ['car','bus','hospital','hotel','gun','bomb','horse','fox','table','bowl','guitar','piano']\n",
    "w2sim = ds.k_most_similar(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'car', 1.0),\n",
       " (u'history', -0.15115789972759958),\n",
       " (u'text', -0.26226840947899999),\n",
       " (u'market', -0.36955511780147104),\n",
       " (u'economy', -0.38972338959638458),\n",
       " (u'year', -0.40801622790322367),\n",
       " (u'business', -0.45524213911197303),\n",
       " (u'case', -0.46700191886219455),\n",
       " (u'price', -0.48427401476008269),\n",
       " (u'time', -0.49814133092182583),\n",
       " (u'period', -0.51142309447448309),\n",
       " (u'society', -0.51356120088132173),\n",
       " (u'education', -0.51563384962275005),\n",
       " (u'order', -0.51637872789032258),\n",
       " (u'community', -0.51717864757827092),\n",
       " (u'father', -0.5208925301408166),\n",
       " (u'game', -0.52261063187878454),\n",
       " (u'law', -0.54649911286173602),\n",
       " (u'function', -0.54762230441489623),\n",
       " (u'growth', -0.55112694656448535)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2sim['car']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Evaluator: BLESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bless_evaluator(simMatrix=None, indexers=[None,None]):\n",
    "    wordToIndex, indexToWord = indexers\n",
    "    path = '/Users/jacobsw/Desktop/IMPLEMENTATION_CAMP/CODE/BASIC_TOPICS/DISTRIBUTIONAL_SEMANTICS/ASSIGNMENT_03/BLESS_part.txt'\n",
    "    with open(path,'rb') as f:\n",
    "        bless = f.readlines()\n",
    "    bless = [line.split('\\t') for line in bless] # split into (concept, _, relation, relatum).\n",
    "    crPairs = [(c.split('-')[0],r.split('-')[0],rel) for c,_,rel,r in bless]\n",
    "    posPairs = [(c,r) for c,r,rel in crPairs if rel=='hyper']\n",
    "    negPairs = [(c,r) for c,r,rel in crPairs if rel=='mero']\n",
    "    \n",
    "    return [map(lambda (c,r):(c,r,simMatrix[wordToIndex[c]][wordToIndex[r]]), posPairs),\n",
    "            map(lambda (c,r):(c,r,simMatrix[wordToIndex[c]][wordToIndex[r]]), negPairs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 ms, sys: 4.95 ms, total: 18.8 ms\n",
      "Wall time: 19.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ds.build_similarity_matrix(cosine) # comment out if has been computed previously.\n",
    "posEval, negEval = bless_evaluator(ds.simMatrix, indexers=[ds.wordToIndex, ds.indexToWord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Cosine): \n",
      "[('turtle', 'pet', 0.45376598151239955), ('clarinet', 'artifact', 0.14487351849329463), ('battleship', 'ship', 0.75923317149541025), ('television', 'object', 0.066853681127950509), ('castle', 'building', 0.43025187595393954)]\n",
      "Average Cosine:  0.395534739769\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(posEval, 5)\n",
    "print \"Average Cosine: \", np.mean([cosineVal for _,_,cosineVal in posEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Cosine): \n",
      "[('van', 'gear', 0.41969753514314606), ('bus', 'plate', 0.11086664248226279), ('sword', 'point', -0.00087320182307122163), ('helicopter', 'wheel', 0.49287113499505791), ('car', 'door', 0.36863646621214197)]\n",
      "Average Cosine:  0.363030447208\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Cosine): \"\n",
    "print random.sample(negEval, 5)\n",
    "print \"Average Cosine: \", np.mean([cosineVal for _,_,cosineVal in negEval])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15 ms, sys: 15.4 ms, total: 30.4 ms\n",
      "Wall time: 60 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ds.build_similarity_matrix(jaccard) # comment out if has been computed previously.\n",
    "                                      # CAUTION: jaccard is slow!!!\n",
    "posEval, negEval = bless_evaluator(ds.simMatrix, indexers=[ds.wordToIndex, ds.indexToWord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Jaccard): \n",
      "[('glove', 'garment', -1.2472914246383657), ('castle', 'home', -0.91276277360885272), ('jet', 'plane', -1.6013089320024307), ('sheep', 'animal', -0.82145541582443582), ('jet', 'craft', -2.075346455557209)]\n",
      "Average Cosine:  -1.32821427543\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Jaccard): \"\n",
    "print random.sample(posEval, 5)\n",
    "print \"Average Cosine: \", np.mean([jaccardVal for _,_,jaccardVal in posEval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Evaluation on Positive Relations (Jaccard): \n",
      "[('bus', 'mirror', -0.98619981130401546), ('cathedral', 'dome', -0.99491967294843175), ('restaurant', 'garden', -0.61123827919885343), ('cathedral', 'tower', -1.0782011960757136), ('castle', 'hall', -1.1341032130484032)]\n",
      "Average Cosine:  -1.36918847379\n"
     ]
    }
   ],
   "source": [
    "print \"Examples of Evaluation on Positive Relations (Jaccard): \"\n",
    "print random.sample(negEval, 5)\n",
    "print \"Average Cosine: \", np.mean([jaccardVal for _,_,jaccardVal in negEval])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### D. Accessories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## II. Word Embedding Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... extracting data\n",
      "... cleaning data\n",
      "... building token list and vocabulary\n",
      "... saving top 50-frequent in vocabulary\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "sents = load_wiki()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'anarchism'],\n",
       " [u'anarchism', u'philosophy', u'theory', u'attitude', u'state'],\n",
       " [u'anarchist', u'criterion', u'anarchism', u'criterion'],\n",
       " [u'oxford',\n",
       "  u'companion',\n",
       "  u'philosophy',\n",
       "  u'position',\n",
       "  u'anarchist',\n",
       "  u'anarchist',\n",
       "  u'share',\n",
       "  u'family',\n",
       "  u'resemblance'],\n",
       " [u'type', u'tradition', u'anarchism']]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
