{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hindle (1990)\n",
    "\n",
    "* **Statistic**: \n",
    "    * Subj/Obj-V Mutual Information\n",
    "    * Subj/Ojb-V Similarity\n",
    "    * Noun Similarity\n",
    "* **Corpus**:\n",
    "    * Brown (NLTK)\n",
    "* **Parsing**:\n",
    "    * Type: Dependency\n",
    "    * Library: Spacy\n",
    "* **Categorization**:\n",
    "    * Subject\n",
    "    * Object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Subj/Obj-V Mutual Information**\n",
    "\n",
    "    * $C_{subj/obj}(n,v) = log_2\\frac{\\frac{f(n,v)}{N}}{\\frac{f(n)}{N}\\cdot\\frac{f(v)}{N}}$, where $f(n,v)$ is the frequency of noun $n$ occurring as the subj/obj of verb $v$; $f(n)$ the frequency of the noun $n$ occurring as the argument of any verb, and $f(v)$ is the frequency of the verb $v$; $N$ is the count of clauses in the dataset.\n",
    "\n",
    "\n",
    "* **Subj/Obj-V Similarity**\n",
    "\n",
    "    * $ {SIM}_{subj/obj}(v_i,n_j,n_k) = \\begin{cases} \n",
    "    min(C_{subj/obj}(v_i,n_j), C_{subj/obj}(v_i,n_k)), & \\text{if } C_{subj/obj}(v_i,n_j)>0 \\text{ and } C_{subj/obj(v_i,n_k)}(v_i,n_k)>0, \\\\\n",
    "    abs(max(C_{subj/obj}(v_i,n_j), C_{subj/obj}(v_i,n_k))), & \\text{if } C_{subj/obj}(v_i,n_j)<0 \\text{ and } C_{subj/obj(v_i,n_k)}(v_i,n_k)<0, \\\\\n",
    "    0, & \\text{otherwise}\n",
    "    \\end{cases} $\n",
    "\n",
    "\n",
    "* **Noun Similarity**\n",
    "\n",
    "    * $ {SIM}(n_1,n_2) = \\sum_{i=1}^V {SIM}_{subj}(v_i,n_1,n_2) + {SIM}_{obj}(v_i,n_1,n_2) $, where $V$ is the number of unique verbs.\n",
    "    \n",
    "\n",
    "**NB**: In practice, the so-called *noun* in Hindle (1990) includes more than 'NOUN'-tagged words. For instance, *it* and *much*, which are considered in the paper, are tagged 'PRON' and 'ADV' respectively in brown. Therefore, in actually counting, we extract all 'arguments' instead, which is defined as the constituents which are in either 'nsubj' or 'dobj' relationship with the main verb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Extract Obj-V & Subj-V Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from spacy.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frequency_matrices():\n",
    "    \"\"\"\n",
    "    FUNCTION:\n",
    "        computes subj-V & obj-V frequency matrices with: \n",
    "          - rows: verbs\n",
    "          - cols: subj/obj\n",
    "        returns \n",
    "          - verb-to-index dictionary\n",
    "          - subj-to-index dictionary\n",
    "          - obj-to-index dictionary\n",
    "          - subj-V frequency matrix\n",
    "          - obj-V frequency matrix\n",
    "          - size of dataset\n",
    "          - argument counts\n",
    "          - verb counts\n",
    "    \"\"\"\n",
    "    # extract info from corpus\n",
    "    sents = [' '.join(sent) for sent in brown.sents()]\n",
    "    tagged_words = brown.tagged_words(tagset='universal')\n",
    "    w2t = defaultdict(str,{w:t for w,t in tagged_words})\n",
    "    \n",
    "    # parser corpus\n",
    "    parser = English()\n",
    "    parsed_corpus = [parser(sent) for sent in sents]\n",
    "    \n",
    "    # find subj/obj-v pairs\n",
    "    subj_v_pairs = []\n",
    "    obj_v_pairs = []\n",
    "    arguments = []\n",
    "    verbs = []\n",
    "    for sent in parsed_corpus:\n",
    "        for token in sent:\n",
    "            if w2t[token.head.orth_]=='VERB':\n",
    "                arguments.append(token.orth_)\n",
    "                if token.dep_=='nsubj':\n",
    "                    subj_v_pairs.append((token.orth_,token.head.orth_))\n",
    "                elif token.dep_=='dobj':\n",
    "                    obj_v_pairs.append((token.orth_,token.head.orth_))\n",
    "                else: pass\n",
    "            elif w2t[token.orth_]=='VERB':\n",
    "                verbs.append(token.orth_)\n",
    "            else: continue\n",
    "    \n",
    "    # \"standardize\" subjs, objs and verbs by stemming\n",
    "    subj_v_pairs = [(porter.stem(subj).lower(),porter.stem(v)) \n",
    "                     for subj,v in subj_v_pairs]\n",
    "    obj_v_pairs = [(porter.stem(obj).lower(),porter.stem(v)) \n",
    "                    for obj,v in obj_v_pairs]\n",
    "    arguments = [porter.stem(n).lower() for n in arguments]\n",
    "    verbs = [porter.stem(v).lower() for v in verbs]\n",
    "    \n",
    "    # build pred/arg-to-index dictionaries\n",
    "    verb_vocab = list({verb for _,verb in subj_v_pairs+obj_v_pairs})\n",
    "    subj_vocab = list({subj for subj,_ in subj_v_pairs})\n",
    "    obj_vocab = list({obj for obj,_ in obj_v_pairs})\n",
    "    v2i = defaultdict(int, {v:i for i,v in enumerate(verb_vocab)})\n",
    "    s2i = defaultdict(int, {s:i for i,s in enumerate(subj_vocab)})\n",
    "    o2i = defaultdict(int, {o:i for i,o in enumerate(obj_vocab)})\n",
    "    \n",
    "    # build dependency matrix\n",
    "    subj_v_matrix = np.zeros((len(subj_vocab),len(verb_vocab)))\n",
    "    obj_v_matrix = np.zeros((len(obj_vocab),len(verb_vocab)))\n",
    "    for s,v in subj_v_pairs:\n",
    "        subj_v_matrix[s2i[s]][v2i[v]] += 1\n",
    "    for o,v in obj_v_pairs:\n",
    "        obj_v_matrix[o2i[o]][v2i[v]] += 1 \n",
    "    \n",
    "    return v2i, s2i, o2i, subj_v_matrix, obj_v_matrix, len(sents), Counter(arguments), Counter(verbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 48s, sys: 1.81 s, total: 1min 49s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "v2i, s2i, o2i, subj_v_matrix, obj_v_matrix, N, argument_counts, verb_counts = frequency_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Compute Noun Similarities\n",
    "\n",
    "* Subj/Obj-V Mutual Information Matrices: $C_{subj/obj}(n,v)$,\n",
    "* Subj/Ojb-V Similarity Tensors: ${SIM}_{subj/obj}(v,n_j,n_k)$,\n",
    "* Noun Similarity Matrix: ${SIM}(n_j,n_k)$.\n",
    "\n",
    "**NB**: For one-time computation, one may compute all the tensors and matrix, but will take a LONG time to compute on a large corpus, so in the demo here, we only compute the Mutual Information Matrices, and compute Noun Similarities \"on the spot\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "path = \"/Users/jacobsw/Desktop/FALL_2016/LIN389C_RSCH_COMPLING/BAYESIAN/CODE/COMPUTED/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log2 = lambda x: np.log2(x) if x!=0 else np.log2(1e-20)\n",
    "div = lambda x,y: 0. if y==0 else x/y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mi_matrices(v2i, s2i, o2i, s_v, o_v, \n",
    "                      N, arg_c, v_c):\n",
    "    \n",
    "    mi_subj = lambda n,v: log2( div( (s_v[s2i[n]][v2i[v]]/N), (arg_c[n]/N)*(v_c[v]/N) ) )\n",
    "    mi_obj = lambda n,v: log2( div( (o_v[o2i[n]][v2i[v]]/N), (arg_c[n]/N)*(v_c[v]/N) ) )\n",
    "    \n",
    "    c_subj = np.zeros((len(s2i),len(v2i)))\n",
    "    c_obj = np.zeros((len(o2i),len(v2i)))\n",
    "    for v in v2i.keys():\n",
    "        for s in s2i.keys():\n",
    "            c_subj[s2i[s]][v2i[v]] = mi_subj(s,v)\n",
    "        for o in o2i.keys():\n",
    "            c_obj[o2i[o]][v2i[v]] = mi_obj(o,v)\n",
    "    \n",
    "    return c_subj, c_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 34s, sys: 881 ms, total: 3min 35s\n",
      "Wall time: 3min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "c_subj, c_obj = build_mi_matrices(v2i, s2i, o2i, subj_v_matrix, obj_v_matrix, N, argument_counts, verb_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat-bread:  8.46740428124\n",
      "eat-chicken:  7.54986644144\n",
      "eat-much:  3.53929719973\n",
      "eat-it:  -0.22084602789\n"
     ]
    }
   ],
   "source": [
    "print 'eat-bread: ', c_obj[o2i['bread']][v2i['eat']]\n",
    "print 'eat-chicken: ', c_obj[o2i['chicken']][v2i['eat']]\n",
    "print 'eat-much: ', c_obj[o2i['much']][v2i['eat']]\n",
    "print 'eat-it: ', c_obj[o2i['it']][v2i['eat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IO\n",
    "\n",
    "# with open(path+'mi_matrices.pickle', 'wb') as f:\n",
    "#     cPickle.dump((c_subj,c_obj), f)\n",
    "# with open(path+'mi_matrices.pickle', 'rb') as f:\n",
    "#     c_subj,c_obj = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noun_sim(v2i, s2i, o2i, c_subj, c_obj, n_1, n_2):\n",
    "    \n",
    "    def sim_vnn(v, n_1, n_2, n2i, c_m): \n",
    "        # n2i: s2i or o2i\n",
    "        # c_m: c_subj or c_obj\n",
    "        c_vn_1 = c_m[n2i[n_1]][v2i[v]]\n",
    "        c_vn_2 = c_m[n2i[n_2]][v2i[v]]\n",
    "        if c_vn_1>0 and c_vn_2>0:\n",
    "            return min(c_vn_1, c_vn_2)\n",
    "        elif c_vn_1<0 and c_vn_2<0: \n",
    "            return abs(max(c_vn_1, c_vn_2))\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    return sum(sim_vnn(v,n_1,n_2,s2i,c_subj)+sim_vnn(v,n_1,n_2,o2i,c_obj) \n",
    "               for v in v2i.iterkeys()) / N \n",
    "\n",
    "# NB: the similarity score, as described in Hindle (1990), \n",
    "#     is not normalized by the size of the dataset (i.e. N),  \n",
    "#     however since the score is in positive correlation with the\n",
    "#     size of the corpus, it makes sense to normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_k_similar(arg, k, mode='sim'):\n",
    "    # mode: sim or dissim\n",
    "    # assuming v2i, s2i, o2i, c_subj, c_obj\n",
    "    args = s2i.keys()+o2i.keys()\n",
    "    sim = [(arg_i,noun_sim(v2i,s2i,o2i,c_subj,c_obj,arg,arg_i)) for arg_i in args]\n",
    "    sim.sort(key=itemgetter(1),reverse=True)\n",
    "    return sim[:k] if mode=='sim' else sim[-k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 25s, sys: 2.01 s, total: 3min 27s\n",
      "Wall time: 3min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'boat', 9.0157302624390283),\n",
       " (u'boat', 9.0157302624390283),\n",
       " (u'lie', 9.0134328801042809),\n",
       " (u'lie', 9.0134328801042809),\n",
       " (u'consumm', 9.0134138960425663),\n",
       " (u'consumm', 9.0134138960425663),\n",
       " (u'samo', 9.013388596750465),\n",
       " (u'magnif', 9.013388596750465),\n",
       " (u'spacecraft', 9.013388596750465),\n",
       " (u'samo', 9.013388596750465)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "find_k_similar('boat',10, mode='sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 59s, sys: 3.12 s, total: 5min 2s\n",
      "Wall time: 5min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'they', 8.2263245083546899),\n",
       " (u'they', 8.2263245083546899),\n",
       " (u'which', 8.205074035718523),\n",
       " (u'which', 8.205074035718523),\n",
       " (u'that', 8.20115843012535),\n",
       " (u'that', 8.20115843012535),\n",
       " (u'it', 7.8154963171502496),\n",
       " (u'it', 7.8154963171502496),\n",
       " (u'he', 7.7851723220975719),\n",
       " (u'he', 7.7851723220975719)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "find_k_similar('boat',10, mode='dissim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
