{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Similarity Measures II: KB Syn-Sem Family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Contents\n",
    "\n",
    "* I. Corpora (SpaCy Preprocessing for Lemmatization)\n",
    "    * MSR Paraphrase Corpus (for evaluation)\n",
    "    * Brown Corpus (for computing info content of words)\n",
    "    * WordNet (for computing word similarity)\n",
    "* II. Word Similarity\n",
    "* III. Sentence Similarity\n",
    "* IV. Word-Order Similarity\n",
    "* V. Overall Sentence Similarity (Linear Combination of Sent & Word Similarities)\n",
    "* VI. Evaluation (Not Using MSR for now, computationally expensive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MSR Paraphrase Corpus\n",
    "* NLTK WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. MSR Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = \"/Users/jacobsw/Desktop/WORK/OJO_CODE/SENTENCE_SIMILARITIES/CORPORA/paraphrase/msr_paraphrase_train.txt\"\n",
    "test_path = \"/Users/jacobsw/Desktop/WORK/OJO_CODE/SENTENCE_SIMILARITIES/CORPORA/paraphrase/msr_paraphrase_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quality</th>\n",
       "      <th>#1 ID</th>\n",
       "      <th>#2 ID</th>\n",
       "      <th>#1 String</th>\n",
       "      <th>#2 String</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>702876</td>\n",
       "      <td>702977</td>\n",
       "      <td>Amrozi accused his brother, whom he called the...</td>\n",
       "      <td>Referring to him as only the witness, Amrozi a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2108705</td>\n",
       "      <td>2108831</td>\n",
       "      <td>Yucaipa owned Dominick's before selling the ch...</td>\n",
       "      <td>Yucaipa bought Dominick's in 1995 for $693 mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1330381</td>\n",
       "      <td>1330521</td>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10, the ship's owners had published an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3344667</td>\n",
       "      <td>3344648</td>\n",
       "      <td>Around 0335 GMT, Tab shares were up 19 cents, ...</td>\n",
       "      <td>Tab shares jumped 20 cents, or 4.6%, to set a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1236820</td>\n",
       "      <td>1236712</td>\n",
       "      <td>The stock rose $2.11, or about 11 percent, to ...</td>\n",
       "      <td>PG&amp;E Corp. shares jumped $1.63 or 8 percent to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Quality    #1 ID    #2 ID  \\\n",
       "0        1   702876   702977   \n",
       "1        0  2108705  2108831   \n",
       "2        1  1330381  1330521   \n",
       "3        0  3344667  3344648   \n",
       "4        1  1236820  1236712   \n",
       "\n",
       "                                           #1 String  \\\n",
       "0  Amrozi accused his brother, whom he called the...   \n",
       "1  Yucaipa owned Dominick's before selling the ch...   \n",
       "2  They had published an advertisement on the Int...   \n",
       "3  Around 0335 GMT, Tab shares were up 19 cents, ...   \n",
       "4  The stock rose $2.11, or about 11 percent, to ...   \n",
       "\n",
       "                                           #2 String  \n",
       "0  Referring to him as only the witness, Amrozi a...  \n",
       "1  Yucaipa bought Dominick's in 1995 for $693 mil...  \n",
       "2  On June 10, the ship's owners had published an...  \n",
       "3  Tab shares jumped 20 cents, or 4.6%, to set a ...  \n",
       "4  PG&E Corp. shares jumped $1.63 or 8 percent to...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_table(train_path, encoding='utf-8-sig')\n",
    "df_test = pd.read_table(test_path, encoding='utf-8-sig')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4076, 5)\n",
      "(1725, 5)\n"
     ]
    }
   ],
   "source": [
    "print df_train.shape\n",
    "print df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Quality                                                      1\n",
       "#1 ID                                                   702876\n",
       "#2 ID                                                   702977\n",
       "#1 String    Amrozi accused his brother, whom he called the...\n",
       "#2 String    Referring to him as only the witness, Amrozi a...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.ix[0] # NB: index Quality is actually weirdly 'ï»¿Quality', using '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Amrozi accused his brother, whom he called the witness, of deliberately distorting his evidence.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.ix[0]['#1 String']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_msr(df, indexer):\n",
    "    \n",
    "    X_dic, Y_dic = defaultdict(lambda x: defaultdict(list)), \\\n",
    "                   defaultdict(lambda x: defaultdict(list))\n",
    "    \n",
    "    for i in indexer:\n",
    "        \n",
    "        entry_dic = defaultdict(list)\n",
    "        s1, s2 = df.ix[i]['#1 String'][:-1], \\\n",
    "                 df.ix[i]['#2 String'][:-1] \n",
    "                # get rid of period, which causes problem in distinguishing identical tokens.\n",
    "        \n",
    "        parsed_s1, parsed_s2 = parser(unicode(s1)), parser(unicode(s2))\n",
    "        \n",
    "        entry_dic['s1'] = [token.orth_ for token in parsed_s1]\n",
    "        entry_dic['s2'] = [token.orth_ for token in parsed_s2]\n",
    "        entry_dic['s1_lm'] = [token.lemma_ for token in parsed_s1]\n",
    "        entry_dic['s2_lm'] = [token.lemma_ for token in parsed_s2] \n",
    "#         parsed_lm_s1, parsed_lm_s2 = parser(' '.join(entry_dic['s1_lm'])), \\\n",
    "#                                     parser(' '.join(entry_dic['s2_lm'])) # parse on lemmas.\n",
    "        \n",
    "#         entry_dic['s1_dep_lm'] = dep_lemmas(parsed_lm_s1) # for dep lemma features.\n",
    "#         entry_dic['s2_dep_lm'] = dep_lemmas(parsed_lm_s2)\n",
    "#         entry_dic['s1_dep_tk'] = dep_tokens(parsed_s1) # for dep token features.\n",
    "#         entry_dic['s2_dep_tk'] = dep_tokens(parsed_s2) \n",
    "#         entry_dic['s1_root_lm'] = get_root(parsed_lm_s1)\n",
    "#         entry_dic['s2_root_lm'] = get_root(parsed_lm_s2)\n",
    "#         entry_dic['s1_root_tk'] = get_root(parsed_s1)\n",
    "#         entry_dic['s2_root_tk'] = get_root(parsed_s2)\n",
    "\n",
    "        entry_dic['s1_id'] = df.ix[i]['#1 ID'] # for error analysis later.\n",
    "        entry_dic['s2_id'] = df.ix[i]['#2 ID']\n",
    "        X_dic[i] = entry_dic\n",
    "        Y_dic[i] = df.ix[i]['Quality']\n",
    "    \n",
    "    return X_dic, Y_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.3 s, sys: 161 ms, total: 18.5 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, Y_train = parse_msr(df_train, df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.37 s, sys: 57.3 ms, total: 7.42 s\n",
      "Wall time: 7.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test, Y_test = parse_msr(df_test, df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# msr_sents, msr_words = parse_msr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print msr_sents[0]\n",
    "# print\n",
    "# print msr_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Brown Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_brown():\n",
    "    \n",
    "    sents = brown.sents()\n",
    "    parsed_sents = [parser(' '.join(sent)) for sent in sents]\n",
    "    lemma_words = [token.lemma_ for parsed_sent in parsed_sents for token in parsed_sent]\n",
    "    \n",
    "    return lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 44s, sys: 830 ms, total: 1min 44s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "brown_words = parse_brown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1188973"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(brown_words)\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Word Similarity with Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Math**\n",
    "\n",
    "* **Li et al. (2006)'s WordNet Word Similarity**\n",
    "    * Equation: $SIM(w_1,w_2) = e^{-\\alpha l}\\cdot \\frac{e^{\\beta h}-e^{-\\beta h}}{e^{\\beta h}+e^{-\\beta h}}$ (cf. ibid.:14,(5)).\n",
    "    * Breakdown: The similarity between $w_1$ and $w_2$ is the product of the following functions:\n",
    "        * Path Length Function: $f(l) = e^{-\\alpha l}$\n",
    "        * Subsumer Depth Function: $g(h) = \\frac{e^{\\beta h}-e^{-\\beta h}}{e^{\\beta h}+e^{-\\beta h}}$\n",
    "    * Measures:\n",
    "        * Path Length: (cf. ibid.:13)\n",
    "            * $0$ if $w_1$ and $w_2$ are in the same synset.\n",
    "            * $1$ if $w_1$ and $w_2$ are not in the same synset but the synset for $w_1$ and $w_2$ contain one or more common words.\n",
    "            * *shortest path length* according to WordNet if neither of the above is true.\n",
    "        * Subsumer Depth: (cf. ibid.:14)\n",
    "            * \"Words at upper layers of hierarchical semantic nets have more general concepts and less semantic similarity between words than words at lower layers. Therefore $g(h)$ should increase monotonically with respect to the subsumer depth\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmas = lambda synset: frozenset(str(lemma.name()) for lemma in synset.lemmas()\n",
    "                         if '_' not in str(lemma.name())) # there are lemmas like 'domestic_dog'.\n",
    "div = lambda x,y: x/y if y!=0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PATH_LEN_CACHE = {}\n",
    "def path_len(w1, w2):\n",
    "    \n",
    "    if (w1,w2) in PATH_LEN_CACHE: \n",
    "        return PATH_LEN_CACHE[(w1,w2)]\n",
    "        \n",
    "    w1synsets, w2synsets = wn.synsets(w1), wn.synsets(w2)\n",
    "    w1syns = {lemmas(syn) for syn in w1synsets}\n",
    "    w2syns = {lemmas(syn) for syn in w2synsets}\n",
    "    \n",
    "    for syn in w1syns.union(w2syns):\n",
    "        if w1 in syn and w2 in syn:\n",
    "            return 0\n",
    "    for w1syn in w1syns:\n",
    "        for w2syn in w2syns:\n",
    "            if w1syn.intersection(w2syn):\n",
    "                return 1\n",
    "    pls = []\n",
    "    for w1syn in w1synsets:\n",
    "        for w2syn in w2synsets:\n",
    "            pl = w1syn.shortest_path_distance(w2syn)\n",
    "            if pl!=None: pls.append(pl)\n",
    "    \n",
    "    PATH_LEN_CACHE[(w1,w2)] = 50 if len(pls)==0 else min(pls)\n",
    "    \n",
    "    return PATH_LEN_CACHE[(w1,w2)] # to penalize non-related words\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 Âµs, sys: 2 Âµs, total: 7 Âµs\n",
      "Wall time: 10 Âµs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "path_len('dog','cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SUBSUMER_CACHE = {}\n",
    "def subsumer_depth(w1, w2):\n",
    "    \n",
    "    if (w1,w2) in SUBSUMER_CACHE: \n",
    "        return SUBSUMER_CACHE[(w1,w2)]    \n",
    "    \n",
    "    w1synsets, w2synsets = wn.synsets(w1), wn.synsets(w2)\n",
    "    subsumers = []\n",
    "    for w1syn in w1synsets:\n",
    "        for w2syn in w2synsets:\n",
    "            subsumers += w1syn.common_hypernyms(w2syn)\n",
    "    subsumers = list(set(subsumers))\n",
    "    \n",
    "    depths = [subsumer.min_depth() for subsumer in subsumers] \n",
    "    \n",
    "    SUBSUMER_CACHE[(w1,w2)] = 0 if len(depths)==0 else max(depths) # penalizes no-subsumer case.\n",
    "    \n",
    "    return SUBSUMER_CACHE[(w1,w2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 Âµs, sys: 1e+03 ns, total: 6 Âµs\n",
      "Wall time: 9.06 Âµs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "subsumer_depth('dog','cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORD_SIM_CACHE = {}\n",
    "def word_sim(w1, w2, alpha=.2, beta=.45):\n",
    "    \n",
    "    if (w1,w2) in WORD_SIM_CACHE:\n",
    "        return WORD_SIM_CACHE[(w1,w2)]\n",
    "    \n",
    "    l, h = path_len(w1,w2), subsumer_depth(w1,w2)\n",
    "    \n",
    "    WORD_SIM_CACHE[(w1,w2)] = np.exp(-alpha*l) * \\\n",
    "                              div(np.exp(beta*h)-np.exp(-beta*h), \\\n",
    "                              np.exp(beta*h)+np.exp(-beta*h))\n",
    "        \n",
    "    return WORD_SIM_CACHE[(w1,w2)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.449283876504\n",
      "0.818697350358\n",
      "CPU times: user 124 Âµs, sys: 26 Âµs, total: 150 Âµs\n",
      "Wall time: 138 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print word_sim('dog','cat')\n",
    "print word_sim('dog','canine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Sentence Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Math**\n",
    "\n",
    "* **Sentence Vector $\\check{s}$**:\n",
    "    * Build a vector template $\\check{s}$ the cells of which correspond to the set of distinctive words in two sentences $s_1$, $s_2$, i.e. $\\{w|w\\in s_1\\cup s_2\\}$.\n",
    "    * For $s_1$ and $s_1$, build their vector $\\check{s}_1$ and $\\check{s}_2$ as follows: for each $w$ in $\\check{s}$,\n",
    "        * If $w$ appears in a sentence, set $\\check{s}_{1/2,i} = 1$\n",
    "        * Otherwise, compute $w$'s similarities to all the words in $\\check{s}_{1/2}$, and set $\\check{s}_{1/2,i}$ to be the highest similarity value resulted.\n",
    "    * Each cell of $\\check{s}_{1/2,i}$ is weighted by the corresponding word $w_i$'s *Information Content*, which is computed with $I(w) = \\frac{logp(w)}{log(N+1)} = 1 - \\frac{log(n+1)}{log(N+1)}$, where $n$ is the frequence of $w$ in a corpus (Brown, in this case), $N$ is the size of the corpus. The normalization: $\\check{s}_i = \\check{s}_i\\cdot I(w_i)\\cdot I(\\tilde{w}_i)$, where $\\tilde{w}_i$ is the word entry that is associated with $w_i$ (i.e. either itself, when $w$ is found in a sentence, and $w$'s most similar word otherwise). \n",
    "\n",
    "\n",
    "* **Sentence Similarity**:\n",
    "    * Equation: $SIM(s_1,s_2) = \\frac{\\check{s}_1\\cdot\\check{s}_2}{||\\check{s}_1||\\cdot||\\check{s}_2||}$.\n",
    "    * I.e. Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log = lambda x: np.log(x) if x>0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "I_CACHE = {}\n",
    "def I(w):\n",
    "    if w in I_CACHE:\n",
    "        return I_CACHE[w]\n",
    "    else:\n",
    "        I_CACHE[w] = 1 - div(log(brown_words.count(w)+1),log(N+1))\n",
    "    return I_CACHE[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vec(s1, s2): # assuming s1,s2 are lists of words.\n",
    "    \n",
    "    s_check = list(set(s1).union(set(s2)))\n",
    "    l_check = len(s_check)\n",
    "    s1_check, s2_check = np.zeros(l_check), np.zeros(l_check)\n",
    "    for i,w in enumerate(s_check):\n",
    "        if w in s1: s1_check[i] = 1\n",
    "        else: \n",
    "            idx,most_sim = max(enumerate(s1), key=lambda (j,w_j):word_sim(w,w_j)) # idx: that of w's most sim.\n",
    "            s1_check[i] = word_sim(w,most_sim) * I(w) * I(s1[idx]) # weight by info content\n",
    "        if w in s2: s2_check[i] = 1\n",
    "        else: \n",
    "            idx,most_sim = max(enumerate(s2), key=lambda (j,w_j):word_sim(w,w_j)) \n",
    "            s2_check[i] = word_sim(w,most_sim) * I(w) * I(s2[idx])\n",
    "    \n",
    "    return s1_check, s2_check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_sim(s1, s2):\n",
    "    \n",
    "    s1_vec, s2_vec = vec(s1, s2)\n",
    "    \n",
    "    return div(np.dot(s1_vec,s2_vec),\n",
    "               np.sqrt(np.dot(s1_vec,s1_vec)) * \\\n",
    "               np.sqrt(np.dot(s2_vec,s2_vec)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = X_train[0]['s1']\n",
    "r1 = X_train[0]['s2'] # known to be the paraphrase pairmate to q.\n",
    "r2 = X_train[1]['s1'] # know to be not the paraphrase pairmate to q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.805565926878\n",
      "0.150437483504\n",
      "CPU times: user 5.22 ms, sys: 1.34 ms, total: 6.56 ms\n",
      "Wall time: 5.66 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print sent_sim(q, r1)\n",
    "print sent_sim(q, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Amrozi', u'accused', u'his', u'brother', u',', u'whom', u'he', u'called', u'the', u'witness', u',', u'of', u'deliberately', u'distorting', u'his', u'evidence']\n"
     ]
    }
   ],
   "source": [
    "print q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Word-Order Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Math**\n",
    "\n",
    "* **Order Similarity**:\n",
    "    * Equation: $SIM(s_1,s_2) = 1 - \\frac{||r_1 - r_2||}{||r_1 + r_2||}$ (cf. Li et al. (2006):18,(8)).\n",
    "    * Breakdown: Word order vectors $r_1$ and $r_2$ are computed as follows:\n",
    "        * Build vector template $\\check{s}$ as in section III.\n",
    "        * For $s_1$ and $s_2$, build word order vectors. For each $w$ in $\\check{s}$,\n",
    "            * If $w$ is found in $s_{1/2}$, set $r_{1/2,i}$ to be 1.\n",
    "            * Otherwise, set $r_{1/2,i}$ to be the index of the $w$'s most similar word in $s_{1/2}$.\n",
    "    * Idea: \"... normalized difference of word order\" (cf. ibid.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def order_vec(s1, s2):\n",
    "    \n",
    "    s_check = list(set(s1).union(set(s2)))\n",
    "    l_check = len(s_check)\n",
    "    r1, r2 = np.zeros(l_check), np.zeros(l_check)    \n",
    "    for i,w in enumerate(s_check):\n",
    "        if w in s1:\n",
    "            r1[i] = s1.index(w)\n",
    "        else:\n",
    "            most_sim = max(s1, key=lambda w_j:word_sim(w,w_j)) \n",
    "            r1[i] = s1.index(most_sim)\n",
    "        if w in s2:\n",
    "            r2[i] = s2.index(w)\n",
    "        else:\n",
    "            most_sim = max(s2, key=lambda w_j:word_sim(w,w_j)) \n",
    "            r2[i] = s2.index(most_sim)   \n",
    "            \n",
    "    return r1, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def order_sim(s1, s2):\n",
    "    \n",
    "    r1, r2 = order_vec(s1, s2)\n",
    "    \n",
    "    diff = r1 - r2\n",
    "    norm = r1 + r2\n",
    "    \n",
    "    return 1 - div(np.sqrt(np.dot(diff,diff)),np.sqrt(np.dot(norm,norm)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.671823693459\n",
      "0.406778405642\n",
      "CPU times: user 1.08 ms, sys: 452 Âµs, total: 1.53 ms\n",
      "Wall time: 1.19 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print order_sim(q,r1)\n",
    "print order_sim(q,r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Overall Sentence Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Math**\n",
    "\n",
    "* $SIM(s_1,s_2) = \\delta\\cdot SIM_{sent}(s_1,s_2) + (1-\\delta)\\cdot SIM_{order}(s_1,s_2)$.\n",
    "* $\\delta \\in (0.5,1]$, considering word order's \"... subordinate role in semantic processing\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def overall_sent_sim(s1, s2, delta=.85): # delta is a value between [.5,1]. (cf. Li et al. (2006):20,24)\n",
    "    \n",
    "    return delta*sent_sim(s1,s2) + (1-delta)*order_sim(s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.785504591865\n",
      "0.188888621825\n",
      "CPU times: user 2.09 ms, sys: 1.17 ms, total: 3.26 ms\n",
      "Wall time: 2.55 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print overall_sent_sim(q,r1)\n",
    "print overall_sent_sim(q,r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Li et al. (2006) + Wan et al. (2006)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(X_test_fts, Y_test_fts, model):\n",
    "    y_true = Y_test_fts\n",
    "    y_pred = model.predict(X_test_fts)\n",
    "    print 'Accuracy: %.6f' % accuracy_score(y_true,y_pred)\n",
    "    print\n",
    "    print classification_report(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Wan et al. (2006): Featurized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Amrozi', u'accused', u'his', u'brother', u',', u'whom', u'he', u'called', u'the', u'witness', u',', u'of', u'deliberately', u'distorting', u'his', u'evidence']\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print X_train[0]['s1']; print\n",
    "print Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/Users/jacobsw/Desktop/WORK/OJO_CODE/SENTENCE_SIMILARITIES/DATA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD W06 FEATURES\n",
    "# with open(data_path+'train1.p','rb') as f_train:\n",
    "#     X_train_w06fts, Y_train_w06fts = cPickle.load(f_train)\n",
    "# with open(data_path+'test1.p','rb') as f_test:\n",
    "#     X_test_w06fts, Y_test_w06fts = cPickle.load(f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36.129483428692055, 34.004219697592525, 35.034650597519565, 50.932376695342796, 47.936354536793218, 49.388971340938468, 0.5, 0.4924790605054523, 0.49621103366618263, 0.5, 0.4924790605054523, 0.49621103366618263, 0.5714285714285714, 0.5, 0.5333333333333333, 0.5714285714285714, 0.5, 0.5333333333333333, 13, 13, -1, 1]\n"
     ]
    }
   ],
   "source": [
    "print X_train_w06fts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How does Li et al. (2006) do on MSR on its own?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize(X_train, Y_train, X_test, Y_test, sim):\n",
    "    \n",
    "    X_train_fts, Y_train_fts = [], []\n",
    "    X_test_fts, Y_test_fts = [], []\n",
    "    \n",
    "    print \"... processing train\"\n",
    "    for i,x in X_train.iteritems():\n",
    "        if i!=0 and i%100==0:\n",
    "            print \"    ... processed %d train sentences\" % i\n",
    "        X_train_fts.append(sim(x['s1_lm'],x['s2_lm']))\n",
    "        Y_train_fts.append(Y_train[i])\n",
    "    print \"... processing test\"\n",
    "    for i,x in X_test.iteritems():\n",
    "        if i!=0 and i%100==0:\n",
    "            print \"    ... processed %d test sentences\" % i\n",
    "        X_test_fts.append(sim(x['s1_lm'],x['s2_lm']))\n",
    "        Y_test_fts.append(Y_test[i])  \n",
    "        \n",
    "    return X_train_fts, Y_train_fts, X_test_fts, Y_test_fts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... processing train\n",
      "    ... processed 100 train sentences\n",
      "    ... processed 200 train sentences\n",
      "    ... processed 300 train sentences\n",
      "    ... processed 400 train sentences\n",
      "    ... processed 500 train sentences\n",
      "    ... processed 600 train sentences\n",
      "    ... processed 700 train sentences\n",
      "    ... processed 800 train sentences\n",
      "    ... processed 900 train sentences\n",
      "    ... processed 1000 train sentences\n",
      "    ... processed 1100 train sentences\n",
      "    ... processed 1200 train sentences\n",
      "    ... processed 1300 train sentences\n",
      "    ... processed 1400 train sentences\n",
      "    ... processed 1500 train sentences\n",
      "    ... processed 1600 train sentences\n",
      "    ... processed 1700 train sentences\n",
      "    ... processed 1800 train sentences\n",
      "    ... processed 1900 train sentences\n",
      "    ... processed 2000 train sentences\n",
      "    ... processed 2100 train sentences\n",
      "    ... processed 2200 train sentences\n",
      "    ... processed 2300 train sentences\n",
      "    ... processed 2400 train sentences\n",
      "    ... processed 2500 train sentences\n",
      "    ... processed 2600 train sentences\n",
      "    ... processed 2700 train sentences\n",
      "    ... processed 2800 train sentences\n",
      "    ... processed 2900 train sentences\n",
      "    ... processed 3000 train sentences\n",
      "    ... processed 3100 train sentences\n",
      "    ... processed 3200 train sentences\n",
      "    ... processed 3300 train sentences\n",
      "    ... processed 3400 train sentences\n",
      "    ... processed 3500 train sentences\n",
      "    ... processed 3600 train sentences\n",
      "    ... processed 3700 train sentences\n",
      "    ... processed 3800 train sentences\n",
      "    ... processed 3900 train sentences\n",
      "    ... processed 4000 train sentences\n",
      "... processing test\n",
      "    ... processed 100 test sentences\n",
      "    ... processed 200 test sentences\n",
      "    ... processed 300 test sentences\n",
      "    ... processed 400 test sentences\n",
      "    ... processed 500 test sentences\n",
      "    ... processed 600 test sentences\n",
      "    ... processed 700 test sentences\n",
      "    ... processed 800 test sentences\n",
      "    ... processed 900 test sentences\n",
      "    ... processed 1000 test sentences\n",
      "    ... processed 1100 test sentences\n",
      "    ... processed 1200 test sentences\n",
      "    ... processed 1300 test sentences\n",
      "    ... processed 1400 test sentences\n",
      "    ... processed 1500 test sentences\n",
      "    ... processed 1600 test sentences\n",
      "    ... processed 1700 test sentences\n",
      "CPU times: user 22min 42s, sys: 4.99 s, total: 22min 47s\n",
      "Wall time: 22min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_fts, Y_train_fts, X_test_fts, Y_test_fts = featurize(X_train, Y_train, X_test, Y_test, overall_sent_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CONVERT TO LISTS OF LISTS\n",
    "X_train_fts = [[ft] for ft in X_train_fts]\n",
    "X_test_fts = [[ft] for ft in X_test_fts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7561953789031467]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_fts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SAVE\n",
    "# with open(data_path+'li2006_fts.p','wb') as f:\n",
    "#     cPickle.dump((X_train_fts,Y_train_fts,X_test_fts,Y_test_fts), f)\n",
    "# LOAD\n",
    "# with open(data_path+'li2006_fts.p','rb') as f:\n",
    "#     X_train_fts,Y_train_fts,X_test_fts,Y_test_fts = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_li = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_li.fit(X_train_fts, Y_train_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.732174\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.41      0.51       578\n",
      "          1       0.75      0.90      0.82      1147\n",
      "\n",
      "avg / total       0.72      0.73      0.71      1725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(X_test_fts, Y_test_fts, lr_li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How does Wan et al. (2006) do on MSR on its own?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_wan = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_wan.fit(X_train_w06fts, Y_train_w06fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.732754\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.51      0.56       578\n",
      "          1       0.77      0.85      0.81      1147\n",
      "\n",
      "avg / total       0.72      0.73      0.72      1725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(X_test_w06fts, Y_test_w06fts, lr_wan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wan + Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize_plus(wan_fts, li_fts):\n",
    "    \n",
    "    X_train_wan, Y_train_wan, X_test_wan, Y_test_wan = wan_fts\n",
    "    X_train_li, Y_train_li, X_test_li, Y_test_li = li_fts\n",
    "    \n",
    "    X_train_fts, Y_train_fts = [], []\n",
    "    X_test_fts, Y_test_fts = [], []\n",
    "    \n",
    "    for i,(x_wan,x_li) in enumerate(zip(X_train_wan,X_train_li)):\n",
    "        X_train_fts.append(x_wan+x_li) \n",
    "        Y_train_fts.append(Y_train_li[i])\n",
    "    for i,(x_wan,x_li) in enumerate(zip(X_test_wan,X_test_li)):\n",
    "        X_test_fts.append(x_wan+x_li)\n",
    "        Y_test_fts.append(Y_test_li[i])\n",
    "        \n",
    "    return X_train_fts, Y_train_fts, X_test_fts, Y_test_fts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wan_fts = (X_train_w06fts,Y_train_w06fts,X_test_w06fts,Y_test_w06fts)\n",
    "li_fts = (X_train_fts,Y_train_fts,X_test_fts,Y_test_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.4 ms, sys: 2.08 ms, total: 9.48 ms\n",
      "Wall time: 7.89 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_fts, Y_train_fts, X_test_fts, Y_test_fts = featurize_plus(wan_fts,li_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_wan_li = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_wan_li.fit(X_train_fts, Y_train_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.731594\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.50      0.55       578\n",
      "          1       0.77      0.85      0.81      1147\n",
      "\n",
      "avg / total       0.72      0.73      0.72      1725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(X_test_fts, Y_test_fts, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_linear = svm.SVC(kernel='linear',verbose=3)\n",
    "svm_rbf = svm.SVC(kernel='rbf',verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]CPU times: user 12.8 s, sys: 67.7 ms, total: 12.9 s\n",
      "Wall time: 12.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=3)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "svm_linear.fit(X_train_fts, Y_train_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.734493\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.47      0.54       578\n",
      "          1       0.77      0.87      0.81      1147\n",
      "\n",
      "avg / total       0.72      0.73      0.72      1725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(X_test_fts, Y_test_fts, svm_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]CPU times: user 1.15 s, sys: 9.96 ms, total: 1.16 s\n",
      "Wall time: 1.16 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=3)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "svm_rbf.fit(X_train_fts, Y_train_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.692754\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.30      0.39       578\n",
      "          1       0.72      0.89      0.79      1147\n",
      "\n",
      "avg / total       0.67      0.69      0.66      1725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(X_test_fts, Y_test_fts, svm_rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. OJO Sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ojo_sents = '''\n",
    "What are the quality of schools in this neighborhood?\n",
    "What areas have the best schools?\n",
    "What are the crime statistics in this neighborhood?\n",
    "What are the number of registered sex offenders in this neighborhood?\n",
    "What is the walkability score in this neighborhood?\n",
    "Which neighborhoods have homes that are over 2500 sq ft. \n",
    "What neighborhoods have new construction?\n",
    "Show me pictures of the neighborhood\n",
    "Show me pictures of homes in the neighborhood\n",
    "How bicycle friendly is this neighborhood?\n",
    "What is the median income of this neighborhood?\n",
    "What is the average demographics of this neighborhood? \n",
    "What is the poverty score of this neighborhood?\n",
    "What is the best day of the week to list my home?\n",
    "What is the best month to list a home like mine for the most money and shortest time?\n",
    "How much has my home appreciated?\n",
    "How has appreciation been in my neighborhood vs other neighborhoods?\n",
    "What has the average appreciation in my neighborhood been over the last x years?\n",
    "What has the average appreciation in my school district been over the last x years?\n",
    "What has the average appreciation on my street been over the last x years?\n",
    "Which neighborhoods are best for kids under 10\n",
    "Show me the nearest parks\n",
    "Show me the nearest pools\n",
    "Show me the nearest dog parks\n",
    "Show me the nearest urgent care / emergency room?\n",
    "Show me the nearest fire / police station?\n",
    "Show me the impact of railroad/trains\n",
    "How has appreciation been in this neighborhood vs other neighborhoods?\n",
    "What has the average appreciation in this neighborhood over the last x years?\n",
    "What has the average appreciation in this school district over the last x years?\n",
    "Where can I find a house that is a better fit for me for less money?\n",
    "What is the commute time for this neighborhood?\n",
    "Which neighborhoods have a commute time of less than 30min from [address]\n",
    "I want to live in a low traffic spot\n",
    "Show me diversity of neighborhood\n",
    "Show me historic natural disaster trends for this area\n",
    "Show me historic weather trends for this area\n",
    "Where can I find a house that is a better fit for me for less money?\n",
    "What confidence level does OJO have that I should list my home now?\n",
    "What confidence level does OJO have that I should buy a home right now?\n",
    "How much is my home worth?\n",
    "What confidence level does OJO have that I should buy a home right now?\n",
    "Show me district city government information\n",
    "Which street(s) in this neighborhood have the highest appreciation over x years?\n",
    "What is the expected appreciation for my home over the next x years?\n",
    "Which neighborhood in Austin is expected to appreciate the most over the next x years that have homes similar to what I'm interested in?\n",
    "What areas have mature trees?\n",
    "What areas have the most greenspace?\n",
    "What is the expected appreciation for homes in this area over the next x years?\n",
    "How is this neighborhood impacted by traffic congestion and which time(s) of day?\n",
    "How fast will my home sell?\n",
    "Is this a pet friendly neighborhood?\n",
    "What are the utility costs in this neighborhood?\n",
    "I want to live in a tidy area\n",
    "I want a area where the homes are setback from the streeet\n",
    "Are there complete streets in this neighborhood (connecting sidewalks)?\n",
    "Green building score?\n",
    "Air quality of city/neighborhood?\n",
    "Air quality of home (VOCs, materials)\n",
    "Curbside waster services?\n",
    "Curbside recycling services?\n",
    "Curbside composting services?\n",
    "Average heating/cooling costs?\n",
    "Is sustainable energy availalbe?\n",
    "Show me the impact of flight patterns\n",
    "What are the zoning breakdowns of this neighborhood? (section 8, residtential, mixed used, commercial, etc)?\n",
    "What is the estimated time to sell my home right now?\n",
    "How long does it take to sell a home in my neighborhood right now?\n",
    "How long does it take to sell a home on my street right now?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_mark(s):\n",
    "    return s[:-1] if s.endswith('?') or s.endswith('.') else s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ojo_sents = ojo_sents.split('\\n') # split into list of sent strings.\n",
    "ojo_sents = ojo_sents[1:len(ojo_sents)-1] # get rid of ''s in front and end.\n",
    "ojo_sents = list({drop_mark(sent) for sent in ojo_sents}) # get rid of question mark and duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q1 = 'are the schools in the neighborhood good?'\n",
    "q2 = 'i care the most about the commute time between home and work.'\n",
    "q3 = 'is this a saft neighborhood?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_lemmas(s):\n",
    "    parsed_s = parser(unicode(s))\n",
    "    return [token.lemma_ for token in parsed_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_sim(q, k=5):\n",
    "    \n",
    "    q = to_lemmas(drop_mark(q))\n",
    "    sents = nlargest(k, ojo_sents, key=lambda s: overall_sent_sim(q,s))\n",
    "    \n",
    "    for i,sent in enumerate(sents):\n",
    "        print \"Sim Rank: %d | Sent: %s\" % (i+1,sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim Rank: 1 | Sent: What are the number of registered sex offenders in this neighborhood\n",
      "Sim Rank: 2 | Sent: How has appreciation been in this neighborhood vs other neighborhoods\n",
      "Sim Rank: 3 | Sent: How has appreciation been in my neighborhood vs other neighborhoods\n",
      "Sim Rank: 4 | Sent: What are the quality of schools in this neighborhood\n",
      "Sim Rank: 5 | Sent: What has the average appreciation in my school district been over the last x years\n"
     ]
    }
   ],
   "source": [
    "most_sim(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim Rank: 1 | Sent: What is the best month to list a home like mine for the most money and shortest time\n",
      "Sim Rank: 2 | Sent: Which neighborhood in Austin is expected to appreciate the most over the next x years that have homes similar to what I'm interested in\n",
      "Sim Rank: 3 | Sent: What is the commute time for this neighborhood\n",
      "Sim Rank: 4 | Sent: What is the estimated time to sell my home right now\n",
      "Sim Rank: 5 | Sent: What is the best day of the week to list my home\n"
     ]
    }
   ],
   "source": [
    "most_sim(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim Rank: 1 | Sent: What are the number of registered sex offenders in this neighborhood\n",
      "Sim Rank: 2 | Sent: How has appreciation been in this neighborhood vs other neighborhoods\n",
      "Sim Rank: 3 | Sent: Is this a pet friendly neighborhood\n",
      "Sim Rank: 4 | Sent: What is the walkability score in this neighborhood\n",
      "Sim Rank: 5 | Sent: What are the utility costs in this neighborhood\n"
     ]
    }
   ],
   "source": [
    "most_sim(q3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
