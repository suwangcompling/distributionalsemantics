{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Similarity Measures II: KB Syn-Sem Family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Contents\n",
    "\n",
    "* I. Corpora (SpaCy Preprocessing for Lemmatization)\n",
    "    * MSR Paraphrase Corpus (for evaluation)\n",
    "    * Brown Corpus (for computing info content of words)\n",
    "    * WordNet (for computing word similarity)\n",
    "* II. Word Similarity\n",
    "* III. Sentence Similarity\n",
    "* IV. Word-Order Similarity\n",
    "* V. Overall Sentence Similarity (Linear Combination of Sent & Word Similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MSR Paraphrase Corpus\n",
    "* NLTK WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. MSR Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/Users/jacobsw/Desktop/WORK/OJO_CODE/SENTENCE_SIMILARITIES/CORPORA/paraphrase/msr_paraphrase_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>﻿Sentence ID</th>\n",
       "      <th>String</th>\n",
       "      <th>Author</th>\n",
       "      <th>URL</th>\n",
       "      <th>Agency</th>\n",
       "      <th>Date</th>\n",
       "      <th>Web Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>702876</td>\n",
       "      <td>Amrozi accused his brother, whom he called \"th...</td>\n",
       "      <td>Darren Goodsir</td>\n",
       "      <td>www.theage.com.au</td>\n",
       "      <td>*</td>\n",
       "      <td>June 5 2003</td>\n",
       "      <td>2003/06/04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>702977</td>\n",
       "      <td>Referring to him as only \"the witness\", Amrozi...</td>\n",
       "      <td>Darren Goodsir</td>\n",
       "      <td>www.smh.com.au</td>\n",
       "      <td>Sydney Morning Herald</td>\n",
       "      <td>June 5 2003</td>\n",
       "      <td>2003/06/04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2108705</td>\n",
       "      <td>Yucaipa owned Dominick's before selling the ch...</td>\n",
       "      <td>MICHAEL GIBBS</td>\n",
       "      <td>www.nwherald.com</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>2003/08/23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2108831</td>\n",
       "      <td>Yucaipa bought Dominick's in 1995 for $693 mil...</td>\n",
       "      <td>ALEX VEIGA</td>\n",
       "      <td>www.miami.com</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>2003/08/23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1330381</td>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>Philip Pangalos</td>\n",
       "      <td>www.alertnet.org</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>2003/06/25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ﻿Sentence ID                                             String  \\\n",
       "0        702876  Amrozi accused his brother, whom he called \"th...   \n",
       "1        702977  Referring to him as only \"the witness\", Amrozi...   \n",
       "2       2108705  Yucaipa owned Dominick's before selling the ch...   \n",
       "3       2108831  Yucaipa bought Dominick's in 1995 for $693 mil...   \n",
       "4       1330381  They had published an advertisement on the Int...   \n",
       "\n",
       "            Author                URL                 Agency         Date  \\\n",
       "0   Darren Goodsir  www.theage.com.au                      *  June 5 2003   \n",
       "1   Darren Goodsir     www.smh.com.au  Sydney Morning Herald  June 5 2003   \n",
       "2    MICHAEL GIBBS   www.nwherald.com                      *            *   \n",
       "3       ALEX VEIGA      www.miami.com                      *            *   \n",
       "4  Philip Pangalos   www.alertnet.org                      *            *   \n",
       "\n",
       "     Web Date  \n",
       "0  2003/06/04  \n",
       "1  2003/06/04  \n",
       "2  2003/08/23  \n",
       "3  2003/08/23  \n",
       "4  2003/06/25  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path, delimiter='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10594"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['String'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.\n",
      "\n",
      "Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.\n",
      "\n",
      "Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $2.5 billion.\n",
      "\n",
      "Yucaipa bought Dominick's in 1995 for $693 million and sold it to Safeway for $1.8 billion in 1998.\n",
      "\n",
      "They had published an advertisement on the Internet on June 10, offering the cargo for sale, he added.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = list(df['String'])\n",
    "for i in xrange(5):\n",
    "    print data[i]\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_msr():\n",
    "    \n",
    "    parsed_sents = [parser(unicode(sent.decode('utf8','ignore'))) for sent in data]\n",
    "    lemma_sents = [[token.lemma_ for token in parsed_sent] \n",
    "                   for parsed_sent in parsed_sents]\n",
    "    lemma_words = [lemma for lemma_sent in lemma_sents for lemma in lemma_sent]\n",
    "    \n",
    "    return lemma_sents, lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.7 s, sys: 60 ms, total: 15.7 s\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "msr_sents, msr_words = parse_msr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'amrozi', u'accuse', u'his', u'brother', u',', u'whom', u'he', u'call', u'\"', u'the', u'witness', u'\"', u',', u'of', u'deliberately', u'distort', u'his', u'evidence', u'.']\n",
      "\n",
      "[u'amrozi', u'accuse', u'his', u'brother', u',', u'whom', u'he', u'call', u'\"', u'the']\n"
     ]
    }
   ],
   "source": [
    "print msr_sents[0]\n",
    "print\n",
    "print msr_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Brown Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_brown():\n",
    "    \n",
    "    sents = brown.sents()\n",
    "    parsed_sents = [parser(' '.join(sent)) for sent in sents]\n",
    "    lemma_words = [token.lemma_ for parsed_sent in parsed_sents for token in parsed_sent]\n",
    "    \n",
    "    return lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 36s, sys: 715 ms, total: 1min 37s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "brown_words = parse_brown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1188973"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(brown_words)\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Word Similarity with Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Math**\n",
    "\n",
    "* **Li et al. (2006)'s WordNet Word Similarity**\n",
    "    * Equation: $SIM(w_1,w_2) = e^{-\\alpha l}\\cdot \\frac{e^{\\beta h}-e^{-\\beta h}}{e^{\\beta h}+e^{-\\beta h}}$ (cf. ibid.:14,(5)).\n",
    "    * Breakdown: The similarity between $w_1$ and $w_2$ is the product of the following functions:\n",
    "        * Path Length Function: $f(l) = e^{-\\alpha l}$\n",
    "        * Subsumer Depth Function: $g(h) = \\frac{e^{\\beta h}-e^{-\\beta h}}{e^{\\beta h}+e^{-\\beta h}}$\n",
    "    * Measures:\n",
    "        * Path Length: (cf. ibid.:13)\n",
    "            * $0$ if $w_1$ and $w_2$ are in the same synset.\n",
    "            * $1$ if $w_1$ and $w_2$ are not in the same synset but the synset for $w_1$ and $w_2$ contain one or more common words.\n",
    "            * *shortest path length* according to WordNet if neither of the above is true.\n",
    "        * Subsumer Depth: (cf. ibid.:14)\n",
    "            * \"Words at upper layers of hierarchical semantic nets have more general concepts and less semantic similarity between words than words at lower layers. Therefore $g(h)$ should increase monotonically with respect to the subsumer depth\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmas = lambda synset: frozenset(str(lemma.name()) for lemma in synset.lemmas()\n",
    "                         if '_' not in str(lemma.name())) # there are lemmas like 'domestic_dog'.\n",
    "div = lambda x,y: x/y if y!=0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def path_len(w1, w2):\n",
    "    \n",
    "    w1synsets, w2synsets = wn.synsets(w1), wn.synsets(w2)\n",
    "    w1syns = {lemmas(syn) for syn in w1synsets}\n",
    "    w2syns = {lemmas(syn) for syn in w2synsets}\n",
    "    \n",
    "    for syn in w1syns.union(w2syns):\n",
    "        if w1 in syn and w2 in syn:\n",
    "            return 0\n",
    "    for w1syn in w1syns:\n",
    "        for w2syn in w2syns:\n",
    "            if w1syn.intersection(w2syn):\n",
    "                return 1\n",
    "    pls = []\n",
    "    for w1syn in w1synsets:\n",
    "        for w2syn in w2synsets:\n",
    "            pl = w1syn.shortest_path_distance(w2syn)\n",
    "            if pl!=None: pls.append(pl)\n",
    "\n",
    "    return 50 if len(pls)==0 else min(pls) # to penalize non-related words\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.86 ms, sys: 2.66 ms, total: 11.5 ms\n",
      "Wall time: 9.43 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "path_len('dog','cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subsumer_depth(w1, w2):\n",
    "    \n",
    "    w1synsets, w2synsets = wn.synsets(w1), wn.synsets(w2)\n",
    "    subsumers = []\n",
    "    for w1syn in w1synsets:\n",
    "        for w2syn in w2synsets:\n",
    "            subsumers += w1syn.common_hypernyms(w2syn)\n",
    "    subsumers = list(set(subsumers))\n",
    "    \n",
    "    depths = [subsumer.min_depth() for subsumer in subsumers] \n",
    "    \n",
    "    return 0 if len(depths)==0 else max(depths) # penalizes no-subsumer case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 889 µs, sys: 355 µs, total: 1.24 ms\n",
      "Wall time: 996 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "subsumer_depth('dog','cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_sim(w1, w2, alpha=.2, beta=.45):\n",
    "    \n",
    "    l, h = path_len(w1,w2), subsumer_depth(w1,w2)\n",
    "    \n",
    "    return np.exp(-alpha*l) * \\\n",
    "           div(np.exp(beta*h)-np.exp(-beta*h), \\\n",
    "               np.exp(beta*h)+np.exp(-beta*h))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.449283876504\n",
      "0.818697350358\n"
     ]
    }
   ],
   "source": [
    "print word_sim('dog','cat')\n",
    "print word_sim('dog','canine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Sentence Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Math**\n",
    "\n",
    "* **Sentence Vector $\\check{s}$**:\n",
    "    * Build a vector template $\\check{s}$ the cells of which correspond to the set of distinctive words in two sentences $s_1$, $s_2$, i.e. $\\{w|w\\in s_1\\cup s_2\\}$.\n",
    "    * For $s_1$ and $s_1$, build their vector $\\check{s}_1$ and $\\check{s}_2$ as follows: for each $w$ in $\\check{s}$,\n",
    "        * If $w$ appears in a sentence, set $\\check{s}_{1/2,i} = 1$\n",
    "        * Otherwise, compute $w$'s similarities to all the words in $\\check{s}_{1/2}$, and set $\\check{s}_{1/2,i}$ to be the highest similarity value resulted.\n",
    "    * Each cell of $\\check{s}_{1/2,i}$ is weighted by the corresponding word $w_i$'s *Information Content*, which is computed with $I(w) = \\frac{logp(w)}{log(N+1)} = 1 - \\frac{log(n+1)}{log(N+1)}$, where $n$ is the frequence of $w$ in a corpus (Brown, in this case), $N$ is the size of the corpus. The normalization: $\\check{s}_i = \\check{s}_i\\cdot I(w_i)\\cdot I(\\tilde{w}_i)$, where $\\tilde{w}_i$ is the word entry that is associated with $w_i$ (i.e. either itself, when $w$ is found in a sentence, and $w$'s most similar word otherwise). \n",
    "\n",
    "\n",
    "* **Sentence Similarity**:\n",
    "    * Equation: $SIM(s_1,s_2) = \\frac{\\check{s}_1\\cdot\\check{s}_2}{||\\check{s}_1||\\cdot||\\check{s}_2||}$.\n",
    "    * I.e. Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log = lambda x: np.log(x) if x>0 else np.log(1e-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "I = lambda w: 1 - div(log(brown_words.count(w)+1),log(N+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vec(s1, s2): # assuming s1,s2 are lists of words.\n",
    "    \n",
    "    s_check = list(set(s1).union(set(s2)))\n",
    "    l_check = len(s_check)\n",
    "    s1_check, s2_check = np.zeros(l_check), np.zeros(l_check)\n",
    "    for i,w in enumerate(s_check):\n",
    "        if w in s1: s1_check[i] = 1\n",
    "        else: \n",
    "            idx,most_sim = max(enumerate(s1), key=lambda (j,w_j):word_sim(w,w_j)) # idx: that of w's most sim.\n",
    "            s1_check[i] = word_sim(w,most_sim) * I(w) * I(s1[idx]) # weight by info content\n",
    "        if w in s2: s2_check[i] = 1\n",
    "        else: \n",
    "            idx,most_sim = max(enumerate(s2), key=lambda (j,w_j):word_sim(w,w_j)) \n",
    "            s2_check[i] = word_sim(w,most_sim) * I(w) * I(s2[idx])\n",
    "    \n",
    "    return s1_check, s2_check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_sim(s1, s2):\n",
    "    \n",
    "    s1_vec, s2_vec = vec(s1, s2)\n",
    "    \n",
    "    return div(np.dot(s1_vec,s2_vec),\n",
    "               np.sqrt(np.dot(s1_vec,s1_vec)) * \\\n",
    "               np.sqrt(np.dot(s2_vec,s2_vec)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = msr_sents[0]\n",
    "r1 = msr_sents[1] # known to be the paraphrase pairmate to q.\n",
    "r2 = msr_sents[2] # know to be not the paraphrase pairmate to q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.797807014451\n",
      "0.185647424791\n",
      "CPU times: user 2.54 s, sys: 18 ms, total: 2.56 s\n",
      "Wall time: 2.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print sent_sim(q, r1)\n",
    "print sent_sim(q, r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Word-Order Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Math**\n",
    "\n",
    "* **Order Similarity**:\n",
    "    * Equation: $SIM(s_1,s_2) = 1 - \\frac{||r_1 - r_2||}{||r_1 + r_2||}$ (cf. Li et al. (2006):18,(8)).\n",
    "    * Breakdown: Word order vectors $r_1$ and $r_2$ are computed as follows:\n",
    "        * Build vector template $\\check{s}$ as in section III.\n",
    "        * For $s_1$ and $s_2$, build word order vectors. For each $w$ in $\\check{s}$,\n",
    "            * If $w$ is found in $s_{1/2}$, set $r_{1/2,i}$ to be 1.\n",
    "            * Otherwise, set $r_{1/2,i}$ to be the index of the $w$'s most similar word in $s_{1/2}$.\n",
    "    * Idea: \"... normalized difference of word order\" (cf. ibid.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def order_vec(s1, s2):\n",
    "    \n",
    "    s_check = list(set(s1).union(set(s2)))\n",
    "    l_check = len(s_check)\n",
    "    r1, r2 = np.zeros(l_check), np.zeros(l_check)    \n",
    "    for i,w in enumerate(s_check):\n",
    "        if w in s1:\n",
    "            r1[i] = s1.index(w)\n",
    "        else:\n",
    "            most_sim = max(s1, key=lambda w_j:word_sim(w,w_j)) \n",
    "            r1[i] = s1.index(most_sim)\n",
    "        if w in s2:\n",
    "            r2[i] = s2.index(w)\n",
    "        else:\n",
    "            most_sim = max(s2, key=lambda w_j:word_sim(w,w_j)) \n",
    "            r2[i] = s2.index(most_sim)   \n",
    "            \n",
    "    return r1, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def order_sim(s1, s2):\n",
    "    \n",
    "    r1, r2 = order_vec(s1, s2)\n",
    "    \n",
    "    diff = r1 - r2\n",
    "    norm = r1 + r2\n",
    "    \n",
    "    return 1 - div(np.sqrt(np.dot(diff,diff)),np.sqrt(np.dot(norm,norm)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.702183520613\n",
      "0.452824344835\n",
      "CPU times: user 542 ms, sys: 13.5 ms, total: 556 ms\n",
      "Wall time: 549 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print order_sim(q,r1)\n",
    "print order_sim(q,r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Overall Sentence Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Math**\n",
    "\n",
    "* $SIM(s_1,s_2) = \\delta\\cdot SIM_{sent}(s_1,s_2) + (1-\\delta)\\cdot SIM_{order}(s_1,s_2)$.\n",
    "* $\\delta \\in (0.5,1]$, considering word order's \"... subordinate role in semantic processing\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def overall_sent_sim(s1, s2, delta=.85): # delta is a value between [.5,1]. (cf. Li et al. (2006):20,24)\n",
    "    \n",
    "    return delta*sent_sim(s1,s2) + (1-delta)*order_sim(s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.783463490376\n",
      "0.225723962797\n",
      "CPU times: user 3.3 s, sys: 28.9 ms, total: 3.33 s\n",
      "Wall time: 3.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print overall_sent_sim(q,r1)\n",
    "print overall_sent_sim(q,r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
